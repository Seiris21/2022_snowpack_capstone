{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Installing the requirements that Google Colab doesn't have\n",
    "!pip install timm \n",
    "!pip install wandb --quiet\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of our imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import progress\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need torchvision .8+ for some image reading functionality\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data\n",
    "df = pd.read_csv(f'/home/ubuntu/SnowData/Final_CNN_Dataframe.csv')\n",
    "\n",
    "#Designating which columns are our metadata\n",
    "feature_cols = [col for col in df.columns \n",
    "                if col not in \n",
    "                ['cell_id', 'date', 'MOD10A1_filelocations', 'MYD10A1_filelocations', \n",
    "                 'copernicus_filelocations', 'SWE','sentinel1_filelocation','sentinel2a_filelocation',\n",
    "                 'sentinel2b_filelocation','SWE_Scaled']]\n",
    "                 #,'mean_inversed_swe', 'mean_local_swe', 'median_local_swe', 'max_local_swe', 'min_local_swe',\n",
    "                 #'mean_local_elevation', 'median_local_elevation', 'max_local_elevation', 'min_local_elevation']]\n",
    "\n",
    "#Min max scaling the meta data\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "df[feature_cols] =  scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "#We will create a separate scaler for the targets so that we can transform them back and forth\n",
    "target_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "target_scaler.fit(np.array(df['SWE']).reshape(-1, 1))\n",
    "df['SWE_Scaled'] = target_scaler.transform(np.array(df['SWE']).reshape(-1, 1))\n",
    "\n",
    "tabluar_columns = len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tmp drop two rows until better solution i s found\n",
    "df = df.drop([56955,82314])\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join weather data\n",
    "#a = pd.read_csv('/home/ubuntu/SnowData/Unique_CellIDs_byDate__ASO_50M_SWE_USCALB__with_HRRR_TMP_surface_12h.csv',index_col=0)\n",
    "#b = pd.read_csv('/home/ubuntu/SnowData/Unique_CellIDs_byDate__ASO_50M_SWE_USCALB__with_HRRR_PRATE_surface_12h.csv',index_col=0)\n",
    "#weather = pd.merge(a, b,  how='left', on = ['index','cell_id','geometry','date','month_year'])\n",
    "weather = pd.read_csv('/home/ubuntu/SnowData/GRIDMET_Weather_Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>date</th>\n",
       "      <th>precip_daily</th>\n",
       "      <th>wind_dir_avg</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>wind_vel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASO_50M_SWE_USCAJW_3012</td>\n",
       "      <td>POLYGON ((-119.3966606691139 37.58805474778629...</td>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.00</td>\n",
       "      <td>262.80</td>\n",
       "      <td>275.20</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASO_50M_SWE_USCAJW_3012</td>\n",
       "      <td>POLYGON ((-119.3966606691139 37.58805474778629...</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.00</td>\n",
       "      <td>268.90</td>\n",
       "      <td>286.20</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASO_50M_SWE_USCAJW_3012</td>\n",
       "      <td>POLYGON ((-119.3966606691139 37.58805474778629...</td>\n",
       "      <td>2019-04-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248.00</td>\n",
       "      <td>268.90</td>\n",
       "      <td>285.50</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASO_50M_SWE_USCAJW_3012</td>\n",
       "      <td>POLYGON ((-119.3966606691139 37.58805474778629...</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.42</td>\n",
       "      <td>279.89</td>\n",
       "      <td>294.03</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASO_50M_SWE_USCAJW_3012</td>\n",
       "      <td>POLYGON ((-119.3966606691139 37.58805474778629...</td>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196.00</td>\n",
       "      <td>255.50</td>\n",
       "      <td>267.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380131</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>POLYGON ((-119.31499791313145 37.8226861044099...</td>\n",
       "      <td>2018-05-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.00</td>\n",
       "      <td>271.20</td>\n",
       "      <td>283.10</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380132</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>POLYGON ((-119.31499791313145 37.8226861044099...</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>11.0</td>\n",
       "      <td>233.00</td>\n",
       "      <td>268.70</td>\n",
       "      <td>272.10</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380133</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>POLYGON ((-119.31499791313145 37.8226861044099...</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182.00</td>\n",
       "      <td>278.30</td>\n",
       "      <td>287.40</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380134</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>POLYGON ((-119.31499791313145 37.8226861044099...</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219.00</td>\n",
       "      <td>271.10</td>\n",
       "      <td>288.80</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380135</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>POLYGON ((-119.31499791313145 37.8226861044099...</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.79</td>\n",
       "      <td>268.05</td>\n",
       "      <td>277.71</td>\n",
       "      <td>6.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5380136 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         cell_id  \\\n",
       "0        ASO_50M_SWE_USCAJW_3012   \n",
       "1        ASO_50M_SWE_USCAJW_3012   \n",
       "2        ASO_50M_SWE_USCAJW_3012   \n",
       "3        ASO_50M_SWE_USCAJW_3012   \n",
       "4        ASO_50M_SWE_USCAJW_3012   \n",
       "...                          ...   \n",
       "5380131  ASO_50M_SWE_USCATB_2267   \n",
       "5380132  ASO_50M_SWE_USCATB_2267   \n",
       "5380133  ASO_50M_SWE_USCATB_2267   \n",
       "5380134  ASO_50M_SWE_USCATB_2267   \n",
       "5380135  ASO_50M_SWE_USCATB_2267   \n",
       "\n",
       "                                                  geometry        date  \\\n",
       "0        POLYGON ((-119.3966606691139 37.58805474778629...  2019-03-12   \n",
       "1        POLYGON ((-119.3966606691139 37.58805474778629...  2019-04-18   \n",
       "2        POLYGON ((-119.3966606691139 37.58805474778629...  2019-04-14   \n",
       "3        POLYGON ((-119.3966606691139 37.58805474778629...  2016-07-05   \n",
       "4        POLYGON ((-119.3966606691139 37.58805474778629...  2017-01-25   \n",
       "...                                                    ...         ...   \n",
       "5380131  POLYGON ((-119.31499791313145 37.8226861044099...  2018-05-20   \n",
       "5380132  POLYGON ((-119.31499791313145 37.8226861044099...  2019-05-27   \n",
       "5380133  POLYGON ((-119.31499791313145 37.8226861044099...  2017-08-07   \n",
       "5380134  POLYGON ((-119.31499791313145 37.8226861044099...  2019-07-05   \n",
       "5380135  POLYGON ((-119.31499791313145 37.8226861044099...  2016-05-01   \n",
       "\n",
       "         precip_daily  wind_dir_avg  temp_min  temp_max  wind_vel  \n",
       "0                 0.0        114.00    262.80    275.20      5.00  \n",
       "1                 0.0        146.00    268.90    286.20      3.80  \n",
       "2                 0.0        248.00    268.90    285.50      3.70  \n",
       "3                 0.0        215.42    279.89    294.03      4.00  \n",
       "4                 0.0        196.00    255.50    267.50      3.50  \n",
       "...               ...           ...       ...       ...       ...  \n",
       "5380131           0.0        182.00    271.20    283.10      2.20  \n",
       "5380132          11.0        233.00    268.70    272.10      5.70  \n",
       "5380133           0.4        182.00    278.30    287.40      2.40  \n",
       "5380134           0.0        219.00    271.10    288.80      1.80  \n",
       "5380135           0.0         79.79    268.05    277.71      6.96  \n",
       "\n",
       "[5380136 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = []\n",
    "for i in df['date'].values:\n",
    "    if pd.to_datetime(i).strftime('%Y') in year:\n",
    "        continue\n",
    "    else:\n",
    "        year.append(pd.to_datetime(i).strftime('%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017', '2019', '2016', '2018']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>date</th>\n",
       "      <th>SWE</th>\n",
       "      <th>mean_inversed_swe</th>\n",
       "      <th>mean_local_swe</th>\n",
       "      <th>median_local_swe</th>\n",
       "      <th>max_local_swe</th>\n",
       "      <th>min_local_swe</th>\n",
       "      <th>mean_local_elevation</th>\n",
       "      <th>median_local_elevation</th>\n",
       "      <th>...</th>\n",
       "      <th>MOD10A1_Albedo</th>\n",
       "      <th>MOD10A1_NDSI</th>\n",
       "      <th>MYD10A1_SnowCover</th>\n",
       "      <th>MYD10A1_Albedo</th>\n",
       "      <th>MYD10A1_NDSI</th>\n",
       "      <th>copernicus_filelocations</th>\n",
       "      <th>sentinel1_filelocation</th>\n",
       "      <th>sentinel2a_filelocation</th>\n",
       "      <th>sentinel2b_filelocation</th>\n",
       "      <th>SWE_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022229</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.098915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433309</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13982</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>29.657914</td>\n",
       "      <td>0.272693</td>\n",
       "      <td>0.350860</td>\n",
       "      <td>0.403414</td>\n",
       "      <td>0.574175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265740</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767115</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.207274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15107</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>39.044215</td>\n",
       "      <td>0.356247</td>\n",
       "      <td>0.406833</td>\n",
       "      <td>0.412116</td>\n",
       "      <td>0.441068</td>\n",
       "      <td>0.254693</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450598</td>\n",
       "      <td>0.872834</td>\n",
       "      <td>0.612198</td>\n",
       "      <td>0.348205</td>\n",
       "      <td>0.846093</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.272874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24903</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023880</td>\n",
       "      <td>0.032456</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.113103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045450</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26029</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>34.362943</td>\n",
       "      <td>0.100782</td>\n",
       "      <td>0.138450</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.461957</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369786</td>\n",
       "      <td>0.865329</td>\n",
       "      <td>0.763864</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>0.848192</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.240157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33788</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>20.860187</td>\n",
       "      <td>0.116449</td>\n",
       "      <td>0.122795</td>\n",
       "      <td>0.131818</td>\n",
       "      <td>0.186850</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215258</td>\n",
       "      <td>0.780527</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.255780</td>\n",
       "      <td>0.757579</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.145789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35492</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>25.710933</td>\n",
       "      <td>0.089154</td>\n",
       "      <td>0.121055</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>0.415373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.621020</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.179690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38484</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2018-05-28</td>\n",
       "      <td>2.257307</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270596</td>\n",
       "      <td>0.073802</td>\n",
       "      <td>0.074757</td>\n",
       "      <td>0.395168</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.015776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54559</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-05-09</td>\n",
       "      <td>26.732759</td>\n",
       "      <td>0.058020</td>\n",
       "      <td>0.076426</td>\n",
       "      <td>0.025336</td>\n",
       "      <td>0.244502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275032</td>\n",
       "      <td>0.853569</td>\n",
       "      <td>0.704663</td>\n",
       "      <td>0.257716</td>\n",
       "      <td>0.806836</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.186831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68672</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-05-27</td>\n",
       "      <td>18.472615</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.056658</td>\n",
       "      <td>0.043774</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.031663</td>\n",
       "      <td>0.661631</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399603</td>\n",
       "      <td>0.907529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724002</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.129102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89443</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.033922</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.120197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149528</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91302</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>29.916373</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.128726</td>\n",
       "      <td>0.024024</td>\n",
       "      <td>0.442421</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264474</td>\n",
       "      <td>0.854208</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.362223</td>\n",
       "      <td>0.837303</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.209081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95721</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>25.368814</td>\n",
       "      <td>0.080195</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>0.366217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147915</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309746</td>\n",
       "      <td>0.889146</td>\n",
       "      <td>0.797243</td>\n",
       "      <td>0.272609</td>\n",
       "      <td>0.866137</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.177299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97428</th>\n",
       "      <td>ASO_50M_SWE_USCATB_2267</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>0.055945</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.025576</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.661631</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109188</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       cell_id        date        SWE  mean_inversed_swe  \\\n",
       "7536   ASO_50M_SWE_USCATB_2267  2017-08-16   0.000000           0.022229   \n",
       "13982  ASO_50M_SWE_USCATB_2267  2016-04-26  29.657914           0.272693   \n",
       "15107  ASO_50M_SWE_USCATB_2267  2017-01-29  39.044215           0.356247   \n",
       "24903  ASO_50M_SWE_USCATB_2267  2017-07-27   0.000000           0.023880   \n",
       "26029  ASO_50M_SWE_USCATB_2267  2016-03-26  34.362943           0.100782   \n",
       "33788  ASO_50M_SWE_USCATB_2267  2018-04-23  20.860187           0.116449   \n",
       "35492  ASO_50M_SWE_USCATB_2267  2016-04-07  25.710933           0.089154   \n",
       "38484  ASO_50M_SWE_USCATB_2267  2018-05-28   2.257307           0.000829   \n",
       "54559  ASO_50M_SWE_USCATB_2267  2016-05-09  26.732759           0.058020   \n",
       "68672  ASO_50M_SWE_USCATB_2267  2016-05-27  18.472615           0.032847   \n",
       "89443  ASO_50M_SWE_USCATB_2267  2017-07-17   0.019484           0.024706   \n",
       "91302  ASO_50M_SWE_USCATB_2267  2016-04-01  29.916373           0.094444   \n",
       "95721  ASO_50M_SWE_USCATB_2267  2016-04-16  25.368814           0.080195   \n",
       "97428  ASO_50M_SWE_USCATB_2267  2016-07-08   0.055945           0.013930   \n",
       "\n",
       "       mean_local_swe  median_local_swe  max_local_swe  min_local_swe  \\\n",
       "7536         0.029525          0.005268       0.098915       0.000000   \n",
       "13982        0.350860          0.403414       0.574175       0.000000   \n",
       "15107        0.406833          0.412116       0.441068       0.254693   \n",
       "24903        0.032456          0.004347       0.113103       0.000000   \n",
       "26029        0.138450          0.023817       0.461957       0.015759   \n",
       "33788        0.122795          0.131818       0.186850       0.000232   \n",
       "35492        0.121055          0.024231       0.415373       0.000000   \n",
       "38484        0.000896          0.000000       0.002470       0.000000   \n",
       "54559        0.076426          0.025336       0.244502       0.000000   \n",
       "68672        0.056658          0.043774       0.096784       0.031663   \n",
       "89443        0.033922          0.003886       0.120197       0.000000   \n",
       "91302        0.128726          0.024024       0.442421       0.001854   \n",
       "95721        0.108213          0.024542       0.366217       0.000000   \n",
       "97428        0.013760          0.004696       0.025576       0.000302   \n",
       "\n",
       "       mean_local_elevation  median_local_elevation  ...  MOD10A1_Albedo  \\\n",
       "7536               0.147915                0.184615  ...        0.000000   \n",
       "13982              0.265740                0.184615  ...        0.000000   \n",
       "15107              0.147915                0.184615  ...        0.450598   \n",
       "24903              0.147915                0.184615  ...        0.000000   \n",
       "26029              0.147915                0.184615  ...        0.369786   \n",
       "33788              0.147915                0.184615  ...        0.215258   \n",
       "35492              0.147915                0.184615  ...        0.000000   \n",
       "38484              0.147915                0.184615  ...        0.000000   \n",
       "54559              0.147915                0.184615  ...        0.275032   \n",
       "68672              0.661631                0.692308  ...        0.399603   \n",
       "89443              0.147915                0.184615  ...        0.000000   \n",
       "91302              0.147915                0.184615  ...        0.264474   \n",
       "95721              0.147915                0.184615  ...        0.309746   \n",
       "97428              0.661631                0.692308  ...        0.000000   \n",
       "\n",
       "       MOD10A1_NDSI  MYD10A1_SnowCover  MYD10A1_Albedo  MYD10A1_NDSI  \\\n",
       "7536       0.413564           0.000000        0.000000      0.433309   \n",
       "13982      0.570611           0.000000        0.000000      0.767115   \n",
       "15107      0.872834           0.612198        0.348205      0.846093   \n",
       "24903      0.176000           0.000000        0.000000      0.045450   \n",
       "26029      0.865329           0.763864        0.313841      0.848192   \n",
       "33788      0.780527           0.501567        0.255780      0.757579   \n",
       "35492      0.676421           0.000000        0.000000      0.621020   \n",
       "38484      0.270596           0.073802        0.074757      0.395168   \n",
       "54559      0.853569           0.704663        0.257716      0.806836   \n",
       "68672      0.907529           0.000000        0.000000      0.724002   \n",
       "89443      0.085894           0.000000        0.000000      0.149528   \n",
       "91302      0.854208           0.751667        0.362223      0.837303   \n",
       "95721      0.889146           0.797243        0.272609      0.866137   \n",
       "97428      0.188762           0.000000        0.000000      0.109188   \n",
       "\n",
       "                                copernicus_filelocations  \\\n",
       "7536   /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "13982  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "15107  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "24903  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "26029  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "33788  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "35492  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "38484  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "54559  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "68672  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "89443  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "91302  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "95721  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "97428  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "\n",
       "                                  sentinel1_filelocation  \\\n",
       "7536   /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "13982  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "15107  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "24903  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "26029  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "33788  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "35492  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "38484  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "54559  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "68672  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "89443  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "91302  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "95721  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "97428  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "\n",
       "                                 sentinel2a_filelocation  \\\n",
       "7536   /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "13982  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "15107  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "24903  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "26029  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "33788  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "35492  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "38484  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "54559  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "68672  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "89443  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "91302  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "95721  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "97428  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "\n",
       "                                 sentinel2b_filelocation SWE_Scaled  \n",
       "7536   /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "13982  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.207274  \n",
       "15107  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.272874  \n",
       "24903  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "26029  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.240157  \n",
       "33788  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.145789  \n",
       "35492  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.179690  \n",
       "38484  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.015776  \n",
       "54559  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.186831  \n",
       "68672  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.129102  \n",
       "89443  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000136  \n",
       "91302  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.209081  \n",
       "95721  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.177299  \n",
       "97428  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000391  \n",
       "\n",
       "[14 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['cell_id'] == 'ASO_50M_SWE_USCATB_2267']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class args:\n",
    "    #Overall Args\n",
    "    folder_name = \"/home/ubuntu/SnowData\"\n",
    "\n",
    "    #Keep track of features used in wandb\n",
    "    features = feature_cols\n",
    "    lstm_features = ['precip_daily','wind_dir_avg','temp_min','temp_max','wind_vel']\n",
    "\n",
    "    #Setting the number of CPU workers we are using\n",
    "    num_workers = 12\n",
    "\n",
    "    #Setting the seed so we can replicate\n",
    "    seed = 1212\n",
    "\n",
    "    #Toggle for whether or not we want our model pretrained on imagenet\n",
    "    pretrained = True\n",
    "\n",
    "    #Next we pick the model name with the appropriate shape, img size and output\n",
    "    model_name1 = 'mixnet_s'\n",
    "    model_shape1 = 1536\n",
    "    model_name2 = 'tf_efficientnet_b2_ns'\n",
    "    model_shape2 = 1408 #768 for swin small 1536 for swin large 1792 for efficientnet b4 768 for cait-m-36\n",
    "    imagesize = 224\n",
    "    num_classes = 1\n",
    "    img_channels = 3\n",
    "\n",
    "    #LSTM variables\n",
    "    lstm_hidden = 64\n",
    "    lstm_layers = 2\n",
    "    lstm_seqlen = 10\n",
    "\n",
    "    #Training Args\n",
    "    train_batch_size = 24\n",
    "    val_batch_size = 24\n",
    "    test_batch_size = 24\n",
    "\n",
    "    #Max epochs and number of folds\n",
    "    max_epochs = 100\n",
    "    n_splits = 2\n",
    "\n",
    "    #Optimizer and Scheduler args\n",
    "    loss = 'nn.BCEWithLogitsLoss'\n",
    "    lr = 3e-4\n",
    "    warmup_epochs = 5\n",
    "    weight_decay = 3e-6\n",
    "    eta_min = 0.000001\n",
    "    n_accumulate = 1\n",
    "    T_0 = 25\n",
    "    T_max = 2000\n",
    "\n",
    "    #Callback args\n",
    "    #Minimum number amount of improvement to not trigger patience\n",
    "    min_delta = 0.0\n",
    "    #Number of epochs in a row to wait for improvement\n",
    "    patience = 30\n",
    "\n",
    "#Dataloader Args\n",
    "loaderargs = {'num_workers' : args.num_workers, 'pin_memory': False, 'drop_last': False}\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets are how pytorch knows how to read in the data\n",
    "class SWEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df,ts, test = False, seq_len = 10):\n",
    "        self.df = df\n",
    "        self.seq_len = seq_len\n",
    "        #First we must specify the path to the images\n",
    "        #self.MOD10A1_file_names = df['MOD10A1_filelocations'].values\n",
    "        #self.MYD10A1_file_names = df['MYD10A1_filelocations'].values\n",
    "        self.copernicus_file_names = df['copernicus_filelocations'].values\n",
    "        self.sentinel1_file_names = df['sentinel1_filelocation'].values\n",
    "        self.sentinel2a_file_names = df['sentinel2a_filelocation'].values\n",
    "        self.sentinel2b_file_names = df['sentinel2b_filelocation'].values\n",
    "        #Variables to query time series and output\n",
    "        self.cell_id = df['cell_id'].values\n",
    "        self.date = df['date'].values\n",
    "        ts['date'] = pd.to_datetime(ts['date'])\n",
    "        self.timeseries = ts\n",
    "        #The only transform we want to do right now is the resizing\n",
    "        self._transform = T.Resize(size= (args.imagesize, args.imagesize))\n",
    "        #We specify the tabular feature columns\n",
    "        self.meta = df[feature_cols].values\n",
    "        #Now we specify the targets\n",
    "        self.targets = df['SWE_Scaled'].values\n",
    "        #Finally we specify if this is training or test\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Get the image, scale it to between 0-1 and resize it\n",
    "        copernicus_img_path = self.copernicus_file_names[index]\n",
    "        copernicus_img = read_image(copernicus_img_path,mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        copernicus_img = self._transform(copernicus_img)\n",
    "        \n",
    "        sentinel1_img_path = self.sentinel1_file_names[index]\n",
    "        sentinel1_img = read_image(sentinel1_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel1_img = self._transform(sentinel1_img)\n",
    "        \n",
    "        sentinel2a_img_path = self.sentinel2a_file_names[index]\n",
    "        sentinel2a_img = read_image(sentinel2a_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel2a_img = self._transform(sentinel2a_img)\n",
    "        \n",
    "        sentinel2b_img_path = self.sentinel2b_file_names[index]\n",
    "        sentinel2b_img = read_image(sentinel2b_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel2b_img = self._transform(sentinel2b_img)\n",
    "\n",
    "        #Pull from weather data and generate time-series\n",
    "        #Variables = precip_daily|wind_dir_avg|temp_min|temp_max|wind_vel\n",
    "        date_range = pd.date_range(end=self.date[index], periods=self.seq_len)\n",
    "        \n",
    "        sequence = self.timeseries.loc[(self.timeseries['cell_id'] == self.cell_id[index]) \n",
    "                                       & (self.timeseries.date.isin(date_range))].sort_values('date')\n",
    "        #fill na values\n",
    "        sequence = sequence.fillna(-1)\n",
    "        if list(sequence.shape) != [10,8]:\n",
    "            sequence.date.\n",
    "            print(ts.shape,self.cell_id[index],self.date[index])\n",
    "        \n",
    "        #Drop non-tabular data columns\n",
    "        sequence.drop(['cell_id','geometry','date'],axis=1,inplace=True)\n",
    "            \n",
    "        #ts=[]\n",
    "        #for date in date_range:\n",
    "        #    query = self.timeseries.loc[(self.timeseries['cell_id']==self.cell_id[index]) & (self.timeseries['date']==date)]\n",
    "        #    if not query.empty:\n",
    "        #        tmp = list(query['HRRR_TMP_surface_12h'])[0]\n",
    "        #        prate = list(query['HRRR_PRATE_surface_12h'])[0]\n",
    "        #        if np.isnan(tmp):\n",
    "        #            tmp = -1\n",
    "        #        if np.isnan(prate):\n",
    "        #            prate = -1\n",
    "        #        ts.append([tmp,prate])\n",
    "        #    else:\n",
    "        #        #-1 for missing values as Nan was causing issues, and 0 is a valid value\n",
    "        #        ts.append([-1,-1])\n",
    "        #ts = torch.tensor(ts,dtype=torch.float32)\n",
    "        \n",
    "        ts = torch.tensor(sequence.values.tolist(),dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        #Pull in the features for our batch\n",
    "        meta = self.meta[index, :]\n",
    "        \n",
    "        #Specify the target based on whether this is training or test\n",
    "        if self.test:\n",
    "          target = 0\n",
    "        else:\n",
    "          target = self.targets[index]\n",
    "            \n",
    "        return copernicus_img, sentinel1_img, sentinel2a_img, sentinel2b_img, target, meta , ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Data Loader\n",
    "dloader = DataLoader(SWEDataset(traindf, weather),**loaderargs,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5]) ASO_50M_SWE_USCAKC_1753 2019-04-28\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_1754 2019-04-28\n",
      "torch.Size([2, 5]) ASO_50M_SWE_USCAKC_1755 2019-04-28\n",
      "torch.Size([8, 5]) ASO_50M_SWE_USCAKC_2222 2019-04-28\n",
      "torch.Size([0])torch.Size([1, 5]) ASO_50M_SWE_USCAKC_2223  ASO_50M_SWE_USCAKC_2224 2019-04-28\n",
      "2019-04-28\n",
      "torch.Size([1, 5]) ASO_50M_SWE_USCAKC_2743 2019-04-28\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_2744 2019-04-28\n",
      "torch.Size([6, 5]) ASO_50M_SWE_USCAKC_2944 2019-04-28\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_3242 2019-04-28\n",
      "torch.Size([7, 5]) ASO_50M_SWE_USCAKC_3241 2019-04-28\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_3688 2019-04-28\n",
      "torch.Size([9, 5])  ASO_50M_SWE_USCAKC_36892019-04-28\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_3687 2019-04-28\n",
      "torch.Size([8, 5]) ASO_50M_SWE_USCAKC_4131 2019-04-28\n",
      "torch.Size([3, 5]) ASO_50M_SWE_USCAKC_4650 2019-04-28\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKW_1582 2019-03-24\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKW_1583 2019-03-24\n",
      "torch.Size([1, 5]) ASO_50M_SWE_USCAKW_1584 2019-03-24\n",
      "torch.Size([4, 5]) ASO_50M_SWE_USCAKW_1581 2019-03-24\n",
      "torch.Size([3, 5]) ASO_50M_SWE_USCAKW_2071 2019-03-24\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKW_2072 2019-03-24\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKW_2073 2019-03-24\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_797 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_861 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_925 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1053 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1181 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1185 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1245 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1313 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1377 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1437 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1569 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1633 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1629 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1693 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1761 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1757 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1765 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1825 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1821 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1829 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1885 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1893 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1889 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1930 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_1949 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2013 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2017 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2081 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2077 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2209 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2269 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2277 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2273 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2341 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2405 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2401 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2461 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2465 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2469 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2529 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2525 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2533 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2597 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2589 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2657 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2661 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2721 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2725 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2781 2019-07-05\n",
      "torch.Size([20, 5]) torch.Size([20, 5])ASO_50M_SWE_USCATE_2789 ASO_50M_SWE_USCATE_2785 2019-07-05\n",
      " 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2849 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2853 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_2909 2019-07-05\n",
      "torch.Size([20, 5]) torch.Size([20, 5])ASO_50M_SWE_USCATE_2913 2019-07-05\n",
      " ASO_50M_SWE_USCATE_2917 2019-07-05\n",
      "torch.Size([20, 5])torch.Size([20, 5])  ASO_50M_SWE_USCATE_2977 2019-07-05\n",
      "ASO_50M_SWE_USCATE_2981 \n",
      "2019-07-05torch.Size([20, 5]) ASO_50M_SWE_USCATE_3045 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_30372019-07-05 \n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3050 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3101 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3109 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3114 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3165 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3173 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3169 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3178 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3180 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3229 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3233 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3242 2019-07-05torch.Size([20, 5])\n",
      " ASO_50M_SWE_USCATE_3237 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3244 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3297 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3293 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3301 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3306 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3308 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3365 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3361 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3372 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3425 2019-07-05\n",
      "torch.Size([20, 5])torch.Size([20, 5])  ASO_50M_SWE_USCATE_3429ASO_50M_SWE_USCATE_3434 2019-07-05\n",
      " 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3489 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3493 2019-07-05\n",
      "torch.Size([20, 5]) ASO_50M_SWE_USCATE_3498 2019-07-05\n",
      "torch.Size([2, 5]) ASO_50M_SWE_USCAKC_1755 2019-03-16\n",
      "torch.Size([3, 5]) ASO_50M_SWE_USCAKC_1753torch.Size([0]) 2019-03-16 \n",
      "ASO_50M_SWE_USCAKC_1754 2019-03-16\n",
      "torch.Size([1, 5]) ASO_50M_SWE_USCAKC_2224 2019-03-16\n",
      "torch.Size([8, 5]) ASO_50M_SWE_USCAKC_2222 2019-03-16torch.Size([0]) \n",
      "ASO_50M_SWE_USCAKC_2223 2019-03-16\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_2744 2019-03-16\n",
      "torch.Size([3, 5]) ASO_50M_SWE_USCAKC_2743 2019-03-16\n",
      "torch.Size([8, 5]) ASO_50M_SWE_USCAKC_2944 2019-03-16\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_3242 2019-03-16\n",
      "torch.Size([7, 5]) ASO_50M_SWE_USCAKC_3241 2019-03-16\n",
      "torch.Size([7, 5]) ASO_50M_SWE_USCAKC_3689 2019-03-16\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_3687 2019-03-16\n",
      "torch.Size([0]) ASO_50M_SWE_USCAKC_3688 2019-03-16\n",
      "torch.Size([8, 5]) ASO_50M_SWE_USCAKC_4131 2019-03-16\n",
      "torch.Size([4, 5]) ASO_50M_SWE_USCAKC_4650 2019-03-16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#a,b,c,d,e,f,g = next(iter(dloader))\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#g.size()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (a,b,c,d,e,f,g) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dloader):\n\u001b[1;32m      4\u001b[0m     z\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1152\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1154\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:289\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 289\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:513\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthkey should be a byte string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[43manswer_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m     deliver_challenge(c, authkey)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:757\u001b[0m, in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[0;32m--> 757\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m message[:\u001b[38;5;28mlen\u001b[39m(CHALLENGE)] \u001b[38;5;241m==\u001b[39m CHALLENGE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage = \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m message\n\u001b[1;32m    759\u001b[0m message \u001b[38;5;241m=\u001b[39m message[\u001b[38;5;28mlen\u001b[39m(CHALLENGE):]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:221\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:419\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 419\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:384\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    382\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 384\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#a,b,c,d,e,f,g = next(iter(dloader))\n",
    "#g.size()\n",
    "for step, (a,b,c,d,e,f,g) in enumerate(dloader):\n",
    "    z=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(g.shape)==[24,10,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Lightning Requires that the dataset be formatted as a module\n",
    "class SWEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, traindf, valdf,ts,args, loaderargs):\n",
    "        super().__init__()\n",
    "        #Import our training and validation set, which we will define later\n",
    "        self._train_df = traindf\n",
    "        self._val_df = valdf\n",
    "        self.ts = ts\n",
    "\n",
    "        #Makesure we bring in our args so we can use them\n",
    "        self.args = args\n",
    "        self.loaderargs = loaderargs\n",
    "\n",
    "    #Building the datasets\n",
    "    def __create_dataset(self, train=True):\n",
    "        if train == 'train':\n",
    "          return SWEDataset(self._train_df,self.ts)\n",
    "        else:\n",
    "          return SWEDataset(self._val_df, self.ts)\n",
    "\n",
    "    #Using the datasets to return a dataloader\n",
    "    def train_dataloader(self):\n",
    "        SWE_train = self.__create_dataset(\"train\")\n",
    "        return DataLoader(SWE_train, **self.loaderargs, batch_size=self.args.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        SWE_val = self.__create_dataset(\"val\")\n",
    "        return DataLoader(SWE_val, **self.loaderargs, batch_size=self.args.val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_transforms():\n",
    "    transform = {\n",
    "        \"train\": T.Compose(\n",
    "            [\n",
    "                #T.RandomHorizontalFlip(),\n",
    "                #T.RandomVerticalFlip(),\n",
    "                #T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean = (0.485, 0.456, 0.406), \n",
    "                            std = (0.229, 0.224, 0.225))\n",
    "                \n",
    "            ]\n",
    "        ),\n",
    "        \"val\": T.Compose(\n",
    "            [\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean = (0.485, 0.456, 0.406), \n",
    "                            std = (0.229, 0.224, 0.225))\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "    return transform\n",
    "  \n",
    "\n",
    "def mixup(x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor,x4: torch.Tensor,\n",
    "          y: torch.Tensor, \n",
    "          z = torch.Tensor, alpha: float = 1.0):\n",
    "    assert alpha > 0, \"alpha should be larger than 0\"\n",
    "    assert x1.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x1.size()[0])\n",
    "    mixed_x1 = lam * x1 + (1 - lam) * x1[rand_index, :]\n",
    "    mixed_x2 = lam * x2 + (1 - lam) * x2[rand_index, :]\n",
    "    mixed_x3 = lam * x3 + (1 - lam) * x3[rand_index, :]\n",
    "    mixed_x4 = lam * x4 + (1 - lam) * x4[rand_index, :]\n",
    "    mixed_meta = lam * z + (1 - lam) * z[rand_index, :]\n",
    "    target_a, target_b = y, y[rand_index]\n",
    "    return mixed_x1,mixed_x2,mixed_x3, mixed_x4,mixed_meta, target_a, target_b,  lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class CNNLSTM(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.scaler = target_scaler\n",
    "        self.tabular_columns = tabluar_columns\n",
    "        self._criterion = eval(self.args.loss)()\n",
    "        self.transform = get_default_transforms()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "                \n",
    "        #Tracking\n",
    "        self.trainr2 = R2Score()\n",
    "        self.valr2 = R2Score()\n",
    "        \n",
    "        #Image Models\n",
    "        self.model1 = timm.create_model(args.model_name1, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model2 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model3 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model4 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(input_size = 5,\n",
    "                            hidden_size = args.lstm_hidden,\n",
    "                            num_layers = args.lstm_layers,\n",
    "                            batch_first=True,dropout=.1)\n",
    "        #Possible multiple LSTM layers?\n",
    "        #self.lstm = nn.LSTM(input_size = args.lstm_hidden,\n",
    "        #                    hidden_size = self.hidden_size,\n",
    "        ##                    num_layers = args.lstm_layers,\n",
    "        #                    batch_first=True,dropout=.1)\n",
    "        \n",
    "        #Linear regression layer\n",
    "        self.linear1 = nn.Linear(6415,1024)\n",
    "        self.linear2 = nn.Linear(1024,256)\n",
    "        self.linear3 = nn.Linear(256,args.num_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self,features1,features2,features3,features4,meta,ts):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Image Convolution\n",
    "        #Image Models\n",
    "        features1 = self.model1(features1)                 \n",
    "        features1 = self.relu(features1)\n",
    "        features1 = self.dropout(features1)\n",
    "        \n",
    "        features2 = self.model2(features2)                 \n",
    "        features2 = self.relu(features2)\n",
    "        features2 = self.dropout(features2)\n",
    "        \n",
    "        features3 = self.model3(features3)                 \n",
    "        features3 = self.relu(features3)\n",
    "        features3 = self.dropout(features3)\n",
    "        \n",
    "        features4 = self.model4(features4)                 \n",
    "        features4 = self.relu(features4)\n",
    "        features4 = self.dropout(features4)\n",
    "        \n",
    "\n",
    "        #LSTM\n",
    "        batch_size, seq_len, feature_len = ts.size()\n",
    "        # Initialize hidden state with zeros\n",
    "        \n",
    "        h_0 = torch.zeros(2, batch_size, 64,requires_grad=True).cuda()\n",
    "        c_0 = torch.zeros(2, batch_size, 64,requires_grad=True).cuda()\n",
    "        \n",
    "        f_ts, (final_hidden,final_cell) = self.lstm(ts, (h_0,c_0))\n",
    "        f_ts = f_ts.contiguous().view(batch_size,-1)\n",
    "        \n",
    "        #*************************************************************\n",
    "        #Concatenate meta and image features\n",
    "        features = torch.cat([features1,features2,features3,features4,f_ts,meta],dim=1)\n",
    "        #*************************************************************\n",
    "        \n",
    "        #Linear\n",
    "        features = self.linear1(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        features = self.linear2(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        output = self.linear3(features)\n",
    "        return output\n",
    "    \n",
    "###I DIDN\"T MIX UP TS data\n",
    "    def __share_step(self, batch, mode):\n",
    "        copernicus_img, sentinel1_img, sentinel2a_img, sentinel2b_img, labels, meta,ts = batch\n",
    "        labels = labels.float()\n",
    "        meta = meta.float()\n",
    "        ts = ts.float()\n",
    "        copernicus_img = self.transform[mode](copernicus_img)\n",
    "        sentinel1_img = self.transform[mode](sentinel1_img)\n",
    "        sentinel2a_img = self.transform[mode](sentinel2a_img)\n",
    "        sentinel2b_img = self.transform[mode](sentinel2b_img)\n",
    "\n",
    "        rand_index = torch.rand(1)[0]\n",
    "        \n",
    "        #This is a mixup function\n",
    "        if rand_index < 0.5 and mode == 'train':\n",
    "            copernicus_mixed,sentinel1_mixed,sentinel2a_mixed,sentinel2b_mixed, mixed_meta, target_a, target_b, lam = mixup(\n",
    "                                                          copernicus_img,sentinel1_img,sentinel2a_img,sentinel2b_img,\n",
    "                                                          labels, meta, alpha=0.5)\n",
    "            logits = self.forward(copernicus_mixed,sentinel1_mixed,sentinel2a_mixed,sentinel2b_mixed, mixed_meta,ts).squeeze(1)\n",
    "            loss = self._criterion(logits, target_a) * lam + \\\n",
    "                (1 - lam) * self._criterion(logits, target_b)\n",
    "\n",
    "        else:  \n",
    "          logits = self.forward(copernicus_img,sentinel1_img,sentinel2a_img,sentinel2b_img, meta,ts).squeeze(1)\n",
    "          loss = self._criterion(logits, labels)\n",
    "\n",
    "        pred = torch.from_numpy(self.scaler \\\n",
    "            .inverse_transform(np.array(logits.sigmoid().detach().cpu()) \\\n",
    "            .reshape(-1, 1)))\n",
    "        labels = torch.from_numpy(self.scaler \\\n",
    "            .inverse_transform(np.array(labels.detach().cpu()) \\\n",
    "            .reshape(-1, 1)))\n",
    "        \n",
    "        '''\n",
    "        #This is random noise\n",
    "        elif rand_index > 0.8 and mode == 'train':\n",
    "            images = images + (torch.randn(images.size(0),3,args.imagesize,args.imagesize, \n",
    "                                           dtype = torch.float, device = device)*10)/100\n",
    "            logits = self.forward(images, meta).squeeze(1)\n",
    "            loss = self._criterion(logits, labels)\n",
    "        '''\n",
    "\n",
    "        return loss, pred, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred, labels = self.__share_step(batch, 'train')\n",
    "        self.trainr2(pred.cuda(),labels.cuda())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return {'loss': loss, 'pred': pred, 'labels': labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred, labels = self.__share_step(batch, 'val')\n",
    "        self.valr2(pred.cuda(),labels.cuda())\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {'pred': pred, 'labels': labels}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.log('train_r2_epoch',self.trainr2)\n",
    "        self.__share_epoch_end(outputs, 'train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log('val_r2_epoch',self.valr2)\n",
    "        self.__share_epoch_end(outputs, 'val')\n",
    "\n",
    "    def __share_epoch_end(self, outputs, mode):\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for out in outputs:\n",
    "            pred, label = out['pred'], out['labels']\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "        preds = torch.cat(preds)\n",
    "        labels = torch.cat(labels)\n",
    "        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n",
    "        self.log(f'{mode}_RMSE', metrics)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "        return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": {\n",
    "            \"scheduler\": CosineAnnealingLR(optimizer, T_max = args.T_max, eta_min= args.eta_min),\n",
    "            \"interval\": \"step\",\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"frequency\": 1}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type              | Params\n",
      "--------------------------------------------------\n",
      "0  | _criterion | BCEWithLogitsLoss | 0     \n",
      "1  | relu       | ReLU              | 0     \n",
      "2  | dropout    | Dropout           | 0     \n",
      "3  | trainr2    | R2Score           | 0     \n",
      "4  | valr2      | R2Score           | 0     \n",
      "5  | model1     | EfficientNet      | 2.6 M \n",
      "6  | model2     | EfficientNet      | 7.7 M \n",
      "7  | model3     | EfficientNet      | 7.7 M \n",
      "8  | model4     | EfficientNet      | 7.7 M \n",
      "9  | lstm       | LSTM              | 51.5 K\n",
      "10 | linear1    | Linear            | 6.6 M \n",
      "11 | linear2    | Linear            | 262 K \n",
      "12 | linear3    | Linear            | 257   \n",
      "--------------------------------------------------\n",
      "32.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.6 M    Total params\n",
      "130.339   Total estimated model params size (MB)\n",
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/ubuntu/snowcap/weights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b64dd47a734a41bad039f0fb096547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 7.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/auto_restart.py\", line 474, in _capture_metadata_collate\n    data = default_collate(samples)\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [10, 5] at entry 0 and [1, 5] at entry 6\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_epochs, \n\u001b[1;32m     35\u001b[0m                     gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#                    logger=wandb_logger,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m                                 checkpoint_callback,\n\u001b[1;32m     40\u001b[0m                                 lr_monitor])\n\u001b[1;32m     42\u001b[0m SWE_Datamodule \u001b[38;5;241m=\u001b[39m SWEDataModule(traindf, valdf, weather, args \u001b[38;5;241m=\u001b[39m args, loaderargs \u001b[38;5;241m=\u001b[39m loaderargs)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSWE_Datamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#wandb.finish()\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:156\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_check_val_fx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# skip training and run validation in `on_advance_end`\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m batch_idx, (batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mtrain_data_fetcher\u001b[38;5;241m.\u001b[39mstore_on_device:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_batch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:203\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:270\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     yield_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop_batch()\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# wait for batch to be available.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:300\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_profiler(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfetch_next_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_end(batch, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:550\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:562\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py:96\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1183\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1186\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1229\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 7.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/auto_restart.py\", line 474, in _capture_metadata_collate\n    data = default_collate(samples)\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [10, 5] at entry 0 and [1, 5] at entry 6\n"
     ]
    }
   ],
   "source": [
    "#Not doing kfold, instead separating by year\n",
    "traindf = df[(pd.to_datetime(df['date']).dt.year == int('2019'))|\n",
    "            (pd.to_datetime(df['date']).dt.year == int('2018'))].copy().reset_index(drop=True)\n",
    "valdf = df[(pd.to_datetime(df['date']).dt.year == int('2016'))|\n",
    "            (pd.to_datetime(df['date']).dt.year == int('2017'))].copy().reset_index(drop=True)\n",
    "\n",
    "model = CNNLSTM()\n",
    "\n",
    "modelname = 'cnn-lstm-allweather-noest'\n",
    "\n",
    "#Callbacks\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_RMSE\", min_delta=args.min_delta, patience=args.patience, \n",
    "                                    verbose=False, mode=\"min\")\n",
    "progressbar = TQDMProgressBar(refresh_rate = 10)\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='/home/ubuntu/snowcap/weights', \n",
    "                                      filename= f\"{modelname}_best_weights\", save_top_k=1, monitor=\"val_RMSE\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "#Initialize wandb()\n",
    "#wandb.init(name=modelname,project = \"ASO_Modeling\", entity = \"snowcastshowdown\", job_type='train')\n",
    "\n",
    "#Log model parameters into wandb (args variable dictionary)\n",
    "args_dict = dict(args.__dict__)\n",
    "#pop out non-json-able variables\n",
    "for key in ['__module__','__dict__','__weakref__','__doc__']:\n",
    "    args_dict.pop(key,None)\n",
    "#wandb.config.update(args_dict)\n",
    "\n",
    "\n",
    "#wandb_logger = WandbLogger(log_model = 'all')\n",
    "\n",
    "#wandb_logger.watch(model)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=args.max_epochs, \n",
    "                    gpus=1, \n",
    "#                    logger=wandb_logger,\n",
    "                    callbacks=[early_stop_callback, \n",
    "                                progressbar, \n",
    "                                checkpoint_callback,\n",
    "                                lr_monitor])\n",
    "\n",
    "SWE_Datamodule = SWEDataModule(traindf, valdf, weather, args = args, loaderargs = loaderargs)\n",
    "\n",
    "trainer.fit(model, SWE_Datamodule)\n",
    "\n",
    "#wandb.finish()\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.137 MB of 0.137 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-AdamW</td><td>██▇▇▆▅▄▂▁</td></tr><tr><td>train_loss_step</td><td>█▃▁▃▂▁▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-AdamW</td><td>0.00026</td></tr><tr><td>train_loss_step</td><td>0.42593</td></tr><tr><td>trainer/global_step</td><td>449</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sepimage-cnn-lstm</strong>: <a href=\"https://wandb.ai/snowcastshowdown/ASO_Modeling/runs/j1jm9s5h\" target=\"_blank\">https://wandb.ai/snowcastshowdown/ASO_Modeling/runs/j1jm9s5h</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220323_184320-j1jm9s5h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

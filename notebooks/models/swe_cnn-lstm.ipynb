{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm \n",
    "!pip install wandb --quiet\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of our imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import progress\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    #Overall Args\n",
    "    folder_name = \"/\"\n",
    "  \n",
    "    #Setting the number of CPU workers we are using\n",
    "    num_workers = 4\n",
    "\n",
    "    #Setting the seed so we can replicate\n",
    "    seed = 1212\n",
    "\n",
    "    #Toggle for whether or not we want our model pretrained on imagenet\n",
    "    pretrained = True\n",
    "\n",
    "    #Next we pick the model name with the appropriate shape, img size and output\n",
    "    model_name = 'tf_efficientnet_b4_ns'\n",
    "    model_shape = 1792 #768 for swin small 1536 for swin large 1792 for efficientnet b4 768 for cait-m-36\n",
    "    imagesize = 224\n",
    "    num_classes = 1\n",
    "    #Channels for image stuffed into model\n",
    "    channels = 12\n",
    "    \n",
    "    #LSTM variables\n",
    "    lstm_hidden = 64\n",
    "    lstm_layers = 1\n",
    "\n",
    "    #Training Args\n",
    "    train_batch_size = 32\n",
    "    val_batch_size = 32\n",
    "    test_batch_size = 32\n",
    "\n",
    "    #Max epochs and number of folds\n",
    "    max_epochs = 80\n",
    "    n_splits = 2\n",
    "  \n",
    "    #Optimizer and Scheduler args\n",
    "    loss = 'nn.BCEWithLogitsLoss'\n",
    "    lr = 3e-4\n",
    "    warmup_epochs = 5\n",
    "    weight_decay = 3e-6\n",
    "    eta_min = 0.000001\n",
    "    n_accumulate = 1\n",
    "    T_0 = 25\n",
    "    T_max = 1000\n",
    "\n",
    "    #Callback args\n",
    "    #Minimum number amount of improvement to not trigger patience\n",
    "    min_delta = 0.0\n",
    "    #Number of epochs in a row to wait for improvement\n",
    "    patience = 25\n",
    "\n",
    "#Dataloader Args\n",
    "loaderargs = {'num_workers' : args.num_workers, 'pin_memory': False, 'drop_last': False}\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "seed_everything(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data read-in goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class SWEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, test = False):\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Get the image, scale it to between 0-1 and resize it\n",
    "        MOD10A1_img_path = self.MOD10A1_file_names[index]\n",
    "        MOD10A1_img = read_image(MOD10A1_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        MOD10A1_img = self._transform(MOD10A1_img)\n",
    "\n",
    "        MYD10A1_img_path = self.MYD10A1_file_names[index]\n",
    "        MYD10A1_img = read_image(MYD10A1_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        MYD10A1_img = self._transform(MYD10A1_img)\n",
    "\n",
    "        copernicus_img_path = self.copernicus_file_names[index]\n",
    "        copernicus_img = read_image(copernicus_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        copernicus_img = self._transform(copernicus_img)\n",
    "        \n",
    "        sentinel1_img_path = self.sentinel1_file_names[index]\n",
    "        sentinel1_img = read_image(sentinel1_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel1_img = self._transform(sentinel1_img)\n",
    "\n",
    "        #Concatenate image into [X-channel,R,G,B] image (single)\n",
    "        image = torch.cat((MOD10A1_img,MYD10A1_img,copernicus_img,sentinel1_img),dim=1)\n",
    "        \n",
    "        #Pull in the features for our batch\n",
    "        #meta = self.meta[index, :]\n",
    "        \n",
    "        #Specify the target based on whether this is training or test\n",
    "        #if self.test:\n",
    "        #  target = 0\n",
    "        #else:\n",
    "        #  target = self.targets[index]\n",
    "            \n",
    "        return image, target, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader goes here (Sliding Window for LSTM)\n",
    "#Ideally loader output will need to be X-channel image (all satellite images) plus sequence of tabular data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class CNNLSTM(LightningModule):\n",
    "    def __init__(self)\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.scaler = target_scaler\n",
    "        self.tabular_columns = tabluar_columns\n",
    "        self._criterion = eval(self.args.loss)()\n",
    "        self.transform = get_default_transforms()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=.3)\n",
    "        \n",
    "        #Image Models\n",
    "        self.cnn = timm.create_model(args.model_name, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = args.channels,\n",
    "                                       global_pool='')\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(args.model_shape,args.lstm_hidden,args.lstm_layers,batch_first=True,dropout=.1)\n",
    "        \n",
    "        #Linear regression layer\n",
    "        self.linear1 = nn.Linear(args.lstm_hidden*2,args.lstm_hidden*2)\n",
    "        self.linear2 = nn.Linear(args.lstm_hidden*2,args.num_classes)\n",
    "        \n",
    "    def forward(self,image,meta):\n",
    "        batch_size, timesteps, C, R, G, B = x.size()\n",
    "        #Image reshape\n",
    "        image = image.view(batch_size * timesteps, C, R, G, B)\n",
    "        #Image Convolution\n",
    "        features = self.model(image)\n",
    "        features = self.relu()\n",
    "        \n",
    "        #Image reshape-undo\n",
    "        features = features.view(batch_size, timesteps, -1)\n",
    "        \n",
    "        #*************************************************************\n",
    "        #Concatenate tabular data to flattened image <----- This will need some fiddling to make sure right values are getting matched\n",
    "        #features = features.view(features.size(0), -1)\n",
    "        #features = torch.cat([features,meta],dim=1)\n",
    "        #*************************************************************\n",
    "        \n",
    "        #LSTM\n",
    "        self.lstm.flatten_parameters()\n",
    "        features, _ = self.lstm(features)\n",
    "        \n",
    "        #Linear\n",
    "        features = self.linear1(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        output = self.linear2(features)\n",
    "        return output\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: timm in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (0.5.4)\n",
      "Requirement already satisfied: torch>=1.4 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from timm) (1.10.2)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from timm) (0.11.3)\n",
      "Requirement already satisfied: typing_extensions in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torch>=1.4->timm) (4.1.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torchvision->timm) (1.21.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torchvision->timm) (9.0.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pytorch-lightning in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (1.5.10)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (1.21.2)\n",
      "Requirement already satisfied: torch>=1.7.* in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (1.10.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (2.8.0)\n",
      "Requirement already satisfied: setuptools==59.5.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (59.5.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (4.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (2022.2.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (0.3.1)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (0.7.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-lightning) (4.63.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.0.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.6.2)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm \n",
    "!pip install wandb --quiet\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of our imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import progress\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data\n",
    "df = pd.read_csv(f'/home/ubuntu/SnowData/Final_CNN_Dataframe.csv')\n",
    "\n",
    "#Designating which columns are our metadata\n",
    "feature_cols = [col for col in df.columns \n",
    "                if col not in \n",
    "                ['cell_id', 'date', 'MOD10A1_filelocations', 'MYD10A1_filelocations', \n",
    "                 'copernicus_filelocations', 'SWE','sentinel1_filelocation','sentinel2a_filelocation',\n",
    "                 'sentinel2b_filelocation','SWE_Scaled',\n",
    "                 'mean_inversed_swe', 'mean_local_swe', 'median_local_swe', 'max_local_swe', 'min_local_swe',\n",
    "                 'mean_local_elevation', 'median_local_elevation', 'max_local_elevation', 'min_local_elevation']]\n",
    "\n",
    "#Min max scaling the meta data\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "df[feature_cols] =  scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "#We will create a separate scaler for the targets so that we can transform them back and forth\n",
    "target_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "target_scaler.fit(np.array(df['SWE']).reshape(-1, 1))\n",
    "df['SWE_Scaled'] = target_scaler.transform(np.array(df['SWE']).reshape(-1, 1))\n",
    "\n",
    "tabluar_columns = len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tmp drop two rows until better solution i s found\n",
    "df = df.drop([56955,82314])\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join weather data\n",
    "a = pd.read_csv('/home/ubuntu/SnowData/Unique_CellIDs_byDate__ASO_50M_SWE_USCALB__with_HRRR_TMP_surface_12h.csv',index_col=0)\n",
    "\n",
    "b = pd.read_csv('/home/ubuntu/SnowData/Unique_CellIDs_byDate__ASO_50M_SWE_USCALB__with_HRRR_PRATE_surface_12h.csv',index_col=0)\n",
    "\n",
    "weather = pd.merge(a, b,  how='left', on = ['index','cell_id','geometry','date','month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = []\n",
    "for i in df['date'].values:\n",
    "    if pd.to_datetime(i).strftime('%Y') in year:\n",
    "        continue\n",
    "    else:\n",
    "        year.append(pd.to_datetime(i).strftime('%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017', '2019', '2016', '2018']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>date</th>\n",
       "      <th>SWE</th>\n",
       "      <th>mean_inversed_swe</th>\n",
       "      <th>mean_local_swe</th>\n",
       "      <th>median_local_swe</th>\n",
       "      <th>max_local_swe</th>\n",
       "      <th>min_local_swe</th>\n",
       "      <th>mean_local_elevation</th>\n",
       "      <th>median_local_elevation</th>\n",
       "      <th>...</th>\n",
       "      <th>MOD10A1_Albedo</th>\n",
       "      <th>MOD10A1_NDSI</th>\n",
       "      <th>MYD10A1_SnowCover</th>\n",
       "      <th>MYD10A1_Albedo</th>\n",
       "      <th>MYD10A1_NDSI</th>\n",
       "      <th>copernicus_filelocations</th>\n",
       "      <th>sentinel1_filelocation</th>\n",
       "      <th>sentinel2a_filelocation</th>\n",
       "      <th>sentinel2b_filelocation</th>\n",
       "      <th>SWE_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ASO_50M_SWE_USCAKC_7</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>73.029772</td>\n",
       "      <td>27.858266</td>\n",
       "      <td>30.746571</td>\n",
       "      <td>30.051429</td>\n",
       "      <td>47.944286</td>\n",
       "      <td>19.234286</td>\n",
       "      <td>2380.488</td>\n",
       "      <td>2301.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176204</td>\n",
       "      <td>0.830138</td>\n",
       "      <td>0.806131</td>\n",
       "      <td>0.339499</td>\n",
       "      <td>0.873714</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.510394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ASO_50M_SWE_USCAKC_8</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>55.866164</td>\n",
       "      <td>27.913840</td>\n",
       "      <td>30.746571</td>\n",
       "      <td>30.051429</td>\n",
       "      <td>47.944286</td>\n",
       "      <td>19.234286</td>\n",
       "      <td>2380.488</td>\n",
       "      <td>2301.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161360</td>\n",
       "      <td>0.793747</td>\n",
       "      <td>0.342564</td>\n",
       "      <td>0.128750</td>\n",
       "      <td>0.792446</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.390440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ASO_50M_SWE_USCAKC_9</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>35.951281</td>\n",
       "      <td>27.954850</td>\n",
       "      <td>30.746571</td>\n",
       "      <td>30.051429</td>\n",
       "      <td>47.944286</td>\n",
       "      <td>19.234286</td>\n",
       "      <td>2380.488</td>\n",
       "      <td>2301.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128699</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>0.353559</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.655651</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.251258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ASO_50M_SWE_USCAKC_10</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>14.504536</td>\n",
       "      <td>27.985398</td>\n",
       "      <td>30.746571</td>\n",
       "      <td>30.051429</td>\n",
       "      <td>47.944286</td>\n",
       "      <td>19.234286</td>\n",
       "      <td>2380.488</td>\n",
       "      <td>2301.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.543217</td>\n",
       "      <td>0.232479</td>\n",
       "      <td>0.114443</td>\n",
       "      <td>0.640034</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.101370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ASO_50M_SWE_USCAKC_11</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>0.770748</td>\n",
       "      <td>28.009257</td>\n",
       "      <td>30.746571</td>\n",
       "      <td>30.051429</td>\n",
       "      <td>47.944286</td>\n",
       "      <td>19.234286</td>\n",
       "      <td>2380.488</td>\n",
       "      <td>2301.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>0.408719</td>\n",
       "      <td>0.059303</td>\n",
       "      <td>0.033972</td>\n",
       "      <td>0.245388</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.005387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95194</th>\n",
       "      <td>ASO_50M_SWE_USCATE_3504</td>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>41.849002</td>\n",
       "      <td>24.757985</td>\n",
       "      <td>28.240571</td>\n",
       "      <td>27.872857</td>\n",
       "      <td>40.472857</td>\n",
       "      <td>16.915714</td>\n",
       "      <td>2755.392</td>\n",
       "      <td>2621.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854219</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>0.590823</td>\n",
       "      <td>0.938346</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.292476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95195</th>\n",
       "      <td>ASO_50M_SWE_USCATE_3505</td>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>24.609933</td>\n",
       "      <td>24.908190</td>\n",
       "      <td>28.240571</td>\n",
       "      <td>27.872857</td>\n",
       "      <td>40.472857</td>\n",
       "      <td>16.915714</td>\n",
       "      <td>2755.392</td>\n",
       "      <td>2621.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864160</td>\n",
       "      <td>0.965664</td>\n",
       "      <td>0.631072</td>\n",
       "      <td>0.978420</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.171995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95196</th>\n",
       "      <td>ASO_50M_SWE_USCATE_3506</td>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>41.967672</td>\n",
       "      <td>25.059875</td>\n",
       "      <td>28.240571</td>\n",
       "      <td>27.872857</td>\n",
       "      <td>40.472857</td>\n",
       "      <td>16.915714</td>\n",
       "      <td>2755.392</td>\n",
       "      <td>2621.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845639</td>\n",
       "      <td>0.939891</td>\n",
       "      <td>0.616825</td>\n",
       "      <td>0.963127</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.293305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95197</th>\n",
       "      <td>ASO_50M_SWE_USCATE_3507</td>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>37.977934</td>\n",
       "      <td>25.209928</td>\n",
       "      <td>28.240571</td>\n",
       "      <td>27.872857</td>\n",
       "      <td>40.472857</td>\n",
       "      <td>16.915714</td>\n",
       "      <td>2755.392</td>\n",
       "      <td>2621.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.817995</td>\n",
       "      <td>0.893079</td>\n",
       "      <td>0.536370</td>\n",
       "      <td>0.929036</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.265422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95198</th>\n",
       "      <td>ASO_50M_SWE_USCATE_3508</td>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>19.011998</td>\n",
       "      <td>23.496040</td>\n",
       "      <td>23.876286</td>\n",
       "      <td>25.182857</td>\n",
       "      <td>30.758571</td>\n",
       "      <td>16.915714</td>\n",
       "      <td>2679.192</td>\n",
       "      <td>2621.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.809992</td>\n",
       "      <td>0.885341</td>\n",
       "      <td>0.591288</td>\n",
       "      <td>0.926147</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.132872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54828 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       cell_id        date        SWE  mean_inversed_swe  \\\n",
       "100       ASO_50M_SWE_USCAKC_7  2019-04-28  73.029772          27.858266   \n",
       "101       ASO_50M_SWE_USCAKC_8  2019-04-28  55.866164          27.913840   \n",
       "102       ASO_50M_SWE_USCAKC_9  2019-04-28  35.951281          27.954850   \n",
       "103      ASO_50M_SWE_USCAKC_10  2019-04-28  14.504536          27.985398   \n",
       "104      ASO_50M_SWE_USCAKC_11  2019-04-28   0.770748          28.009257   \n",
       "...                        ...         ...        ...                ...   \n",
       "95194  ASO_50M_SWE_USCATE_3504  2019-05-03  41.849002          24.757985   \n",
       "95195  ASO_50M_SWE_USCATE_3505  2019-05-03  24.609933          24.908190   \n",
       "95196  ASO_50M_SWE_USCATE_3506  2019-05-03  41.967672          25.059875   \n",
       "95197  ASO_50M_SWE_USCATE_3507  2019-05-03  37.977934          25.209928   \n",
       "95198  ASO_50M_SWE_USCATE_3508  2019-05-03  19.011998          23.496040   \n",
       "\n",
       "       mean_local_swe  median_local_swe  max_local_swe  min_local_swe  \\\n",
       "100         30.746571         30.051429      47.944286      19.234286   \n",
       "101         30.746571         30.051429      47.944286      19.234286   \n",
       "102         30.746571         30.051429      47.944286      19.234286   \n",
       "103         30.746571         30.051429      47.944286      19.234286   \n",
       "104         30.746571         30.051429      47.944286      19.234286   \n",
       "...               ...               ...            ...            ...   \n",
       "95194       28.240571         27.872857      40.472857      16.915714   \n",
       "95195       28.240571         27.872857      40.472857      16.915714   \n",
       "95196       28.240571         27.872857      40.472857      16.915714   \n",
       "95197       28.240571         27.872857      40.472857      16.915714   \n",
       "95198       23.876286         25.182857      30.758571      16.915714   \n",
       "\n",
       "       mean_local_elevation  median_local_elevation  ...  MOD10A1_Albedo  \\\n",
       "100                2380.488                 2301.24  ...        0.176204   \n",
       "101                2380.488                 2301.24  ...        0.161360   \n",
       "102                2380.488                 2301.24  ...        0.128699   \n",
       "103                2380.488                 2301.24  ...        0.120098   \n",
       "104                2380.488                 2301.24  ...        0.032358   \n",
       "...                     ...                     ...  ...             ...   \n",
       "95194              2755.392                 2621.28  ...        0.000000   \n",
       "95195              2755.392                 2621.28  ...        0.000000   \n",
       "95196              2755.392                 2621.28  ...        0.000000   \n",
       "95197              2755.392                 2621.28  ...        0.000000   \n",
       "95198              2679.192                 2621.28  ...        0.000000   \n",
       "\n",
       "       MOD10A1_NDSI  MYD10A1_SnowCover  MYD10A1_Albedo  MYD10A1_NDSI  \\\n",
       "100        0.830138           0.806131        0.339499      0.873714   \n",
       "101        0.793747           0.342564        0.128750      0.792446   \n",
       "102        0.736110           0.353559        0.159386      0.655651   \n",
       "103        0.543217           0.232479        0.114443      0.640034   \n",
       "104        0.408719           0.059303        0.033972      0.245388   \n",
       "...             ...                ...             ...           ...   \n",
       "95194      0.854219           0.904700        0.590823      0.938346   \n",
       "95195      0.864160           0.965664        0.631072      0.978420   \n",
       "95196      0.845639           0.939891        0.616825      0.963127   \n",
       "95197      0.817995           0.893079        0.536370      0.929036   \n",
       "95198      0.809992           0.885341        0.591288      0.926147   \n",
       "\n",
       "                                copernicus_filelocations  \\\n",
       "100    /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "101    /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "102    /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "103    /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "104    /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "...                                                  ...   \n",
       "95194  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "95195  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "95196  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "95197  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "95198  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "\n",
       "                                  sentinel1_filelocation  \\\n",
       "100    /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "101    /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "102    /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "103    /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "104    /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "...                                                  ...   \n",
       "95194  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "95195  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "95196  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "95197  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "95198  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "\n",
       "                                 sentinel2a_filelocation  \\\n",
       "100    /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "101    /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "102    /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "103    /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "104    /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "...                                                  ...   \n",
       "95194  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "95195  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "95196  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "95197  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "95198  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "\n",
       "                                 sentinel2b_filelocation SWE_Scaled  \n",
       "100    /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.510394  \n",
       "101    /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.390440  \n",
       "102    /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.251258  \n",
       "103    /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.101370  \n",
       "104    /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.005387  \n",
       "...                                                  ...        ...  \n",
       "95194  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.292476  \n",
       "95195  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.171995  \n",
       "95196  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.293305  \n",
       "95197  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.265422  \n",
       "95198  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.132872  \n",
       "\n",
       "[54828 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[pd.to_datetime(df['date']).dt.year == int('2019')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class args:\n",
    "    #Overall Args\n",
    "    folder_name = \"/home/ubuntu/SnowData\"\n",
    "\n",
    "    #Keep track of features used in wandb\n",
    "    features = feature_cols\n",
    "\n",
    "    #Setting the number of CPU workers we are using\n",
    "    num_workers = 4\n",
    "\n",
    "    #Setting the seed so we can replicate\n",
    "    seed = 1212\n",
    "\n",
    "    #Toggle for whether or not we want our model pretrained on imagenet\n",
    "    pretrained = True\n",
    "\n",
    "    #Next we pick the model name with the appropriate shape, img size and output\n",
    "    model_name1 = 'mixnet_s'\n",
    "    model_shape1 = 1536\n",
    "    model_name2 = 'tf_efficientnet_b2_ns'\n",
    "    model_shape2 = 1408 #768 for swin small 1536 for swin large 1792 for efficientnet b4 768 for cait-m-36\n",
    "    imagesize = 224\n",
    "    num_classes = 1\n",
    "    img_channels = 3\n",
    "\n",
    "    #LSTM variables\n",
    "    lstm_hidden = 64\n",
    "    lstm_layers = 1\n",
    "    lstm_seqlen = 10\n",
    "\n",
    "    #Training Args\n",
    "    train_batch_size = 24\n",
    "    val_batch_size = 24\n",
    "    test_batch_size = 24\n",
    "\n",
    "    #Max epochs and number of folds\n",
    "    max_epochs = 100\n",
    "    n_splits = 2\n",
    "\n",
    "    #Optimizer and Scheduler args\n",
    "    loss = 'nn.BCEWithLogitsLoss'\n",
    "    lr = 3e-4\n",
    "    warmup_epochs = 5\n",
    "    weight_decay = 3e-6\n",
    "    eta_min = 0.000001\n",
    "    n_accumulate = 1\n",
    "    T_0 = 25\n",
    "    T_max = 2000\n",
    "\n",
    "    #Callback args\n",
    "    #Minimum number amount of improvement to not trigger patience\n",
    "    min_delta = 0.0\n",
    "    #Number of epochs in a row to wait for improvement\n",
    "    patience = 30\n",
    "\n",
    "#Dataloader Args\n",
    "loaderargs = {'num_workers' : args.num_workers, 'pin_memory': False, 'drop_last': False}\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets are how pytorch knows how to read in the data\n",
    "class SWEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df,ts, test = False, seq_len = 10):\n",
    "        self.df = df\n",
    "        self.seq_len = seq_len\n",
    "        #First we must specify the path to the images\n",
    "        #self.MOD10A1_file_names = df['MOD10A1_filelocations'].values\n",
    "        #self.MYD10A1_file_names = df['MYD10A1_filelocations'].values\n",
    "        self.copernicus_file_names = df['copernicus_filelocations'].values\n",
    "        self.sentinel1_file_names = df['sentinel1_filelocation'].values\n",
    "        self.sentinel2a_file_names = df['sentinel2a_filelocation'].values\n",
    "        self.sentinel2b_file_names = df['sentinel2b_filelocation'].values\n",
    "        #Variables to query time series and output\n",
    "        self.cell_id = df['cell_id'].values\n",
    "        self.date = df['date'].values\n",
    "        self.timeseries = ts\n",
    "        #The only transform we want to do right now is the resizing\n",
    "        self._transform = T.Resize(size= (args.imagesize, args.imagesize))\n",
    "        #We specify the tabular feature columns\n",
    "        self.meta = df[feature_cols].values\n",
    "        #Now we specify the targets\n",
    "        self.targets = df['SWE_Scaled'].values\n",
    "        #Finally we specify if this is training or test\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Get the image, scale it to between 0-1 and resize it\n",
    "        copernicus_img_path = self.copernicus_file_names[index]\n",
    "        copernicus_img = read_image(copernicus_img_path,mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        copernicus_img = self._transform(copernicus_img)\n",
    "        \n",
    "        sentinel1_img_path = self.sentinel1_file_names[index]\n",
    "        sentinel1_img = read_image(sentinel1_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel1_img = self._transform(sentinel1_img)\n",
    "        \n",
    "        sentinel2a_img_path = self.sentinel2a_file_names[index]\n",
    "        sentinel2a_img = read_image(sentinel2a_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel2a_img = self._transform(sentinel2a_img)\n",
    "        \n",
    "        sentinel2b_img_path = self.sentinel2b_file_names[index]\n",
    "        sentinel2b_img = read_image(sentinel2b_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel2b_img = self._transform(sentinel2b_img)\n",
    "\n",
    "        #Pull from weather data and generate time-series\n",
    "        date_range = pd.date_range(end=self.date[index], periods=self.seq_len)\n",
    "        ts = []\n",
    "        for date in date_range:\n",
    "            query = self.timeseries.loc[(self.timeseries['cell_id']==self.cell_id[index]) & (self.timeseries['date']==self.date[index])]\n",
    "            if not query.empty:\n",
    "                ts.append([list(query['HRRR_TMP_surface_12h'])[0],list(query['HRRR_PRATE_surface_12h'])[0]])\n",
    "            else:\n",
    "                ts.append([np.nan,np.nan])\n",
    "        ts = torch.tensor(ts)\n",
    "        \n",
    "        \n",
    "        #Pull in the features for our batch\n",
    "        meta = self.meta[index, :]\n",
    "        \n",
    "        #Specify the target based on whether this is training or test\n",
    "        if self.test:\n",
    "          target = 0\n",
    "        else:\n",
    "          target = self.targets[index]\n",
    "            \n",
    "        return copernicus_img, sentinel1_img, sentinel2a_img, sentinel2b_img, target, meta , ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Lightning Requires that the dataset be formatted as a module\n",
    "class SWEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, traindf, valdf,ts,args, loaderargs):\n",
    "        super().__init__()\n",
    "        #Import our training and validation set, which we will define later\n",
    "        self._train_df = traindf\n",
    "        self._val_df = valdf\n",
    "        self.ts = ts\n",
    "\n",
    "        #Makesure we bring in our args so we can use them\n",
    "        self.args = args\n",
    "        self.loaderargs = loaderargs\n",
    "\n",
    "    #Building the datasets\n",
    "    def __create_dataset(self, train=True):\n",
    "        if train == 'train':\n",
    "          return SWEDataset(self._train_df,self.ts)\n",
    "        else:\n",
    "          return SWEDataset(self._val_df, self.ts)\n",
    "\n",
    "    #Using the datasets to return a dataloader\n",
    "    def train_dataloader(self):\n",
    "        SWE_train = self.__create_dataset(\"train\")\n",
    "        return DataLoader(SWE_train, **self.loaderargs, batch_size=self.args.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        SWE_val = self.__create_dataset(\"val\")\n",
    "        return DataLoader(SWE_val, **self.loaderargs, batch_size=self.args.val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_transforms():\n",
    "    transform = {\n",
    "        \"train\": T.Compose(\n",
    "            [\n",
    "                #T.RandomHorizontalFlip(),\n",
    "                #T.RandomVerticalFlip(),\n",
    "                #T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                #T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean = (0.485, 0.456, 0.406), \n",
    "                            std = (0.229, 0.224, 0.225))\n",
    "                \n",
    "            ]\n",
    "        ),\n",
    "        \"val\": T.Compose(\n",
    "            [\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean = (0.485, 0.456, 0.406), \n",
    "                            std = (0.229, 0.224, 0.225))\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "    return transform\n",
    "  \n",
    "\n",
    "def mixup(x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor,x4: torch.Tensor,\n",
    "          y: torch.Tensor, \n",
    "          z = torch.Tensor, alpha: float = 1.0):\n",
    "    assert alpha > 0, \"alpha should be larger than 0\"\n",
    "    assert x1.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x1.size()[0])\n",
    "    mixed_x1 = lam * x1 + (1 - lam) * x1[rand_index, :]\n",
    "    mixed_x2 = lam * x2 + (1 - lam) * x2[rand_index, :]\n",
    "    mixed_x3 = lam * x3 + (1 - lam) * x3[rand_index, :]\n",
    "    mixed_x4 = lam * x4 + (1 - lam) * x4[rand_index, :]\n",
    "    mixed_meta = lam * z + (1 - lam) * z[rand_index, :]\n",
    "    target_a, target_b = y, y[rand_index]\n",
    "    return mixed_x1,mixed_x2,mixed_x3, mixed_x4,mixed_meta, target_a, target_b,  lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class CNNLSTM(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.scaler = target_scaler\n",
    "        self.tabular_columns = tabluar_columns\n",
    "        self._criterion = eval(self.args.loss)()\n",
    "        self.transform = get_default_transforms()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=.3)\n",
    "        \n",
    "        self.hidden_size = 2\n",
    "        \n",
    "        \n",
    "        #Image Models\n",
    "        self.model1 = timm.create_model(args.model_name1, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model2 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model3 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model4 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(input_size = 2,\n",
    "                            hidden_size = 64,\n",
    "                            num_layers = args.lstm_layers,\n",
    "                            batch_first=True,dropout=.1)\n",
    "        #Possible multiple LSTM layers?\n",
    "        #self.lstm = nn.LSTM(input_size = args.lstm_hidden,\n",
    "        #                    hidden_size = self.hidden_size,\n",
    "        ##                    num_layers = args.lstm_layers,\n",
    "        #                    batch_first=True,dropout=.1)\n",
    "        \n",
    "        #Linear regression layer\n",
    "        self.linear1 = nn.Linear(6406,1024)\n",
    "        self.linear2 = nn.Linear(1024,256)\n",
    "        self.linear3 = nn.Linear(1024,args.num_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self,features1,features2,features3,features4,meta,ts):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Image Convolution\n",
    "        #Image Models\n",
    "        features1 = self.model1(features1)                 \n",
    "        features1 = self.relu(features1)\n",
    "        features1 = self.dropout(features1)\n",
    "        \n",
    "        features2 = self.model2(features2)                 \n",
    "        features2 = self.relu(features2)\n",
    "        features2 = self.dropout(features2)\n",
    "        \n",
    "        features3 = self.model3(features3)                 \n",
    "        features3 = self.relu(features3)\n",
    "        features3 = self.dropout(features3)\n",
    "        \n",
    "        features4 = self.model4(features4)                 \n",
    "        features4 = self.relu(features4)\n",
    "        features4 = self.dropout(features4)\n",
    "        \n",
    "\n",
    "        #LSTM\n",
    "        batch_size, seq_len, feature_len = ts.size()\n",
    "        # Initialize hidden state with zeros\n",
    "        \n",
    "        h_0 = torch.zeros(1, batch_size, 64,requires_grad=True).cuda()\n",
    "        c_0 = torch.zeros(1, batch_size, 64,requires_grad=True).cuda()\n",
    "        \n",
    "        f_ts, (final_hidden,final_cell) = self.lstm(ts, (h_0,c_0))\n",
    "        f_ts = f_ts.contiguous().view(batch_size,-1)\n",
    "        \n",
    "        #*************************************************************\n",
    "        #Concatenate meta and image features\n",
    "        features = torch.cat([features1,features2,features3,features4,f_ts,meta],dim=1)\n",
    "        #*************************************************************\n",
    "        \n",
    "        #Linear\n",
    "        features = self.linear1(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        features = self.linear2(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        output = self.linear2(features)\n",
    "        return output\n",
    "    \n",
    "###I DIDN\"T MIX UP TS data\n",
    "    def __share_step(self, batch, mode):\n",
    "        copernicus_img, sentinel1_img, sentinel2a_img, sentinel2b_img, labels, meta,ts = batch\n",
    "        labels = labels.float()\n",
    "        meta = meta.float()\n",
    "        ts = ts.float()\n",
    "        copernicus_img = self.transform[mode](copernicus_img)\n",
    "        sentinel1_img = self.transform[mode](sentinel1_img)\n",
    "        sentinel2a_img = self.transform[mode](sentinel2a_img)\n",
    "        sentinel2b_img = self.transform[mode](sentinel2b_img)\n",
    "\n",
    "        rand_index = torch.rand(1)[0]\n",
    "        \n",
    "        #This is a mixup function\n",
    "        if rand_index < 0.5 and mode == 'train':\n",
    "            copernicus_mixed,sentinel1_mixed,sentinel2a_mixed,sentinel2b_mixed, mixed_meta, target_a, target_b, lam = mixup(\n",
    "                                                          copernicus_img,sentinel1_img,sentinel2a_img,sentinel2b_img,\n",
    "                                                          labels, meta, alpha=0.5)\n",
    "            logits = self.forward(copernicus_mixed,sentinel1_mixed,sentinel2a_mixed,sentinel2b_mixed, mixed_meta,ts).squeeze(1)\n",
    "            loss = self._criterion(logits, target_a) * lam + \\\n",
    "                (1 - lam) * self._criterion(logits, target_b)\n",
    "\n",
    "        else:  \n",
    "          logits = self.forward(copernicus_img,sentinel1_img,sentinel2a_img,sentinel2b_img, meta,ts).squeeze(1)\n",
    "          loss = self._criterion(logits, labels)\n",
    "\n",
    "        pred = torch.from_numpy(self.scaler \\\n",
    "            .inverse_transform(np.array(logits.sigmoid().detach().cpu()) \\\n",
    "            .reshape(-1, 1)))\n",
    "        labels = torch.from_numpy(self.scaler \\\n",
    "            .inverse_transform(np.array(labels.detach().cpu()) \\\n",
    "            .reshape(-1, 1)))\n",
    "        \n",
    "        '''\n",
    "        #This is random noise\n",
    "        elif rand_index > 0.8 and mode == 'train':\n",
    "            images = images + (torch.randn(images.size(0),3,args.imagesize,args.imagesize, \n",
    "                                           dtype = torch.float, device = device)*10)/100\n",
    "            logits = self.forward(images, meta).squeeze(1)\n",
    "            loss = self._criterion(logits, labels)\n",
    "        '''\n",
    "\n",
    "        return loss, pred, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred, labels = self.__share_step(batch, 'train')\n",
    "        self.trainr2(pred.cuda(),labels.cuda())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return {'loss': loss, 'pred': pred, 'labels': labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred, labels = self.__share_step(batch, 'val')\n",
    "        self.valr2(pred.cuda(),labels.cuda())\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {'pred': pred, 'labels': labels}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.log('train_r2_epoch',self.trainr2)\n",
    "        self.__share_epoch_end(outputs, 'train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log('val_r2_epoch',self.valr2)\n",
    "        self.__share_epoch_end(outputs, 'val')\n",
    "\n",
    "    def __share_epoch_end(self, outputs, mode):\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for out in outputs:\n",
    "            pred, label = out['pred'], out['labels']\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "        preds = torch.cat(preds)\n",
    "        labels = torch.cat(labels)\n",
    "        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n",
    "        self.log(f'{mode}_RMSE', metrics)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "        return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": {\n",
    "            \"scheduler\": CosineAnnealingLR(optimizer, T_max = args.T_max, eta_min= args.eta_min),\n",
    "            \"interval\": \"step\",\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"frequency\": 1}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | _criterion | BCEWithLogitsLoss | 0     \n",
      "1 | relu       | ReLU              | 0     \n",
      "2 | dropout    | Dropout           | 0     \n",
      "3 | model1     | EfficientNet      | 2.6 M \n",
      "4 | model2     | EfficientNet      | 7.7 M \n",
      "5 | model3     | EfficientNet      | 7.7 M \n",
      "6 | model4     | EfficientNet      | 7.7 M \n",
      "7 | lstm       | LSTM              | 17.4 K\n",
      "8 | linear1    | Linear            | 6.0 M \n",
      "9 | linear2    | Linear            | 1.0 K \n",
      "-------------------------------------------------\n",
      "31.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.7 M    Total params\n",
      "126.760   Total estimated model params size (MB)\n",
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/ubuntu/snowcap/weights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52abfe64502e42db87a2e32930ebd90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (24x6406 and 5830x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_epochs, \n\u001b[1;32m     35\u001b[0m                     gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#                    logger=wandb_logger,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m                                 checkpoint_callback,\n\u001b[1;32m     40\u001b[0m                                 lr_monitor])\n\u001b[1;32m     42\u001b[0m SWE_Datamodule \u001b[38;5;241m=\u001b[39m SWEDataModule(traindf, valdf, weather, args \u001b[38;5;241m=\u001b[39m args, loaderargs \u001b[38;5;241m=\u001b[39m loaderargs)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSWE_Datamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#wandb.finish()\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1311\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar_callback\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m-> 1311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1375\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:110\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_fetcher \u001b[38;5;241m=\u001b[39m dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    106\u001b[0m     dataloader, dataloader_idx\u001b[38;5;241m=\u001b[39mdataloader_idx\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m dl_max_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> 110\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_dataloaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:122\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_step_and_end\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 122\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:217\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:239\u001b[0m, in \u001b[0;36mAccelerator.validation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:219\u001b[0m, in \u001b[0;36mTrainingTypePlugin.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mCNNLSTM.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 155\u001b[0m     loss, pred, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__share_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalr2(pred\u001b[38;5;241m.\u001b[39mcuda(),labels\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mCNNLSTM.__share_step\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m    123\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_criterion(logits, target_a) \u001b[38;5;241m*\u001b[39m lam \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    124\u001b[0m         (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lam) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_criterion(logits, target_b)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[0;32m--> 127\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopernicus_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentinel1_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentinel2a_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentinel2b_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    128\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_criterion(logits, labels)\n\u001b[1;32m    130\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \\\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;241m.\u001b[39minverse_transform(np\u001b[38;5;241m.\u001b[39marray(logits\u001b[38;5;241m.\u001b[39msigmoid()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()) \\\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mCNNLSTM.forward\u001b[0;34m(self, features1, features2, features3, features4, meta, ts)\u001b[0m\n\u001b[1;32m     93\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([features1,features2,features3,features4,f_ts,meta],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#*************************************************************\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#Linear\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(features)\n\u001b[1;32m     99\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (24x6406 and 5830x1024)"
     ]
    }
   ],
   "source": [
    "#Not doing kfold, instead separating by year\n",
    "traindf = df[(pd.to_datetime(df['date']).dt.year == int('2019'))|\n",
    "            (pd.to_datetime(df['date']).dt.year == int('2018'))].copy().reset_index(drop=True)\n",
    "valdf = df[(pd.to_datetime(df['date']).dt.year == int('2016'))|\n",
    "            (pd.to_datetime(df['date']).dt.year == int('2017'))].copy().reset_index(drop=True)\n",
    "\n",
    "model = CNNLSTM()\n",
    "\n",
    "modelname = 'sepimage-cnn-lstm'\n",
    "\n",
    "#Callbacks\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_RMSE\", min_delta=args.min_delta, patience=args.patience, \n",
    "                                    verbose=False, mode=\"min\")\n",
    "progressbar = TQDMProgressBar(refresh_rate = 10)\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='/home/ubuntu/snowcap/weights', \n",
    "                                      filename= f\"{modelname}_best_weights\", save_top_k=1, monitor=\"val_RMSE\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "#Initialize wandb()\n",
    "#wandb.init(name=modelname,project = \"ASO_Modeling\", entity = \"snowcastshowdown\", job_type='train')\n",
    "\n",
    "#Log model parameters into wandb (args variable dictionary)\n",
    "args_dict = dict(args.__dict__)\n",
    "#pop out non-json-able variables\n",
    "for key in ['__module__','__dict__','__weakref__','__doc__']:\n",
    "    args_dict.pop(key,None)\n",
    "#wandb.config.update(args_dict)\n",
    "\n",
    "\n",
    "#wandb_logger = WandbLogger(log_model = 'all')\n",
    "\n",
    "#wandb_logger.watch(model)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=args.max_epochs, \n",
    "                    gpus=1, \n",
    "#                    logger=wandb_logger,\n",
    "                    callbacks=[early_stop_callback, \n",
    "                                progressbar, \n",
    "                                checkpoint_callback,\n",
    "                                lr_monitor])\n",
    "\n",
    "SWE_Datamodule = SWEDataModule(traindf, valdf, weather, args = args, loaderargs = loaderargs)\n",
    "\n",
    "trainer.fit(model, SWE_Datamodule)\n",
    "\n",
    "#wandb.finish()\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

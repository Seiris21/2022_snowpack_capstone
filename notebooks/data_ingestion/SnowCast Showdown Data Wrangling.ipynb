{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seiris21/2022_snowpack_capstone/blob/main/notebooks/data_ingestion/SnowCast%20Showdown%20Data%20Wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQtxwPWn9xYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007073a9-e687-4cc2-f496-299067ab213e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 6s (42.5 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "!apt-get update\n",
        "# Install GDAL and Geopandas\n",
        "!apt-get install libgdal-dev \n",
        "!apt install gdal-bin python-gdal python3-gdal --quiet\n",
        "!apt install python3-rtree --quiet\n",
        "!pip install git+git://github.com/geopandas/geopandas.git --quiet\n",
        "\n",
        "%pip install -U tornado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"dask[complete]\"\n",
        "%pip install \"dask[complete]\" --upgrade"
      ],
      "metadata": {
        "id": "izqO08-7767U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "%pip install pystac_client planetary_computer rasterio xarray-spatial"
      ],
      "metadata": {
        "id": "qEgdiEnI8wh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAeHA6UB-JqZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN1ofW4_-OY8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "from pystac_client import Client\n",
        "import planetary_computer\n",
        "import xarray\n",
        "import dask.dataframe as dd\n",
        "import xrspatial\n",
        "from datashader.transfer_functions import shade, stack\n",
        "from datashader.colors import Elevation\n",
        "from datashader.utils import export_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBvbkksuJz8F"
      },
      "source": [
        "# Data Ingestion and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsIAo5K3_9pv"
      },
      "outputs": [],
      "source": [
        "trainfeatures = pd.read_csv(\"/content/drive/MyDrive/snowcapstone team spring 2022/Competition_Data/ground_measures_train_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhyIte-eARhO"
      },
      "outputs": [],
      "source": [
        "trainfeatures.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8nO3aAqCzSS"
      },
      "outputs": [],
      "source": [
        "trainfeatures = trainfeatures.melt(id_vars=['Unnamed: 0']).dropna().reset_index(drop = True)\n",
        "trainfeatures.rename(columns = {'Unnamed: 0':\"station_id\", \"variable\":\"date\", \"value\":\"SWE\"}, inplace = True)\n",
        "trainfeatures.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These are dates where no stations had information (Discovered after using KNN approach)\n",
        "#Alternative method: Get all dates of cell_id samples, and compare those dates against what each station has for the interpolation\n",
        "nan_dates = ['2013-04-03', '2013-04-29', '2013-05-03', '2013-05-25', '2013-06-01', '2013-06-08', '2016-02-08', '2016-03-26', '2016-04-01', '2016-04-03', '2016-04-04',\n",
        " '2016-04-07', '2016-04-16', '2016-05-09', '2016-05-27', '2016-06-26', '2017-01-28', '2017-01-29', '2018-03-04', '2018-03-30', '2018-03-31', '2018-04-22', '2018-04-23', \n",
        " '2018-04-25', '2018-04-26', '2018-05-24', '2018-05-28', '2018-06-01', '2018-06-02', '2019-03-09', '2019-03-15', '2019-03-16', '2019-03-17', '2019-03-24', '2019-03-25', \n",
        " '2019-03-29', '2019-04-07', '2019-04-08', '2019-04-17', '2019-04-18', '2019-04-19', '2019-04-21', '2019-04-27', '2019-04-28', '2019-05-01', '2019-05-02', '2019-05-03', \n",
        " '2019-06-05', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-13', '2019-06-14', '2019-06-24']"
      ],
      "metadata": {
        "id": "Idoe0HdCX19m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear regression implementation, filling in based on nan_dates list\n",
        "supplement = []\n",
        "\n",
        "#Iterate through all the unique stations\n",
        "for station in trainfeatures['station_id'].unique(): #['CDEC:SSM']: #\n",
        "  #Get subset for this station\n",
        "  subset = trainfeatures[trainfeatures['station_id']==station].copy()\n",
        "  #make filler rows with missing dates\n",
        "  filler = [[station,date,np.nan] for date in nan_dates]\n",
        "  #Append filler rows to subset and sort on date and reset index\n",
        "  subset = subset.append(pd.DataFrame(filler, columns=['station_id','date','SWE'])).sort_values(by='date').reset_index(drop=True)\n",
        "  #print(station,len(subset.index))\n",
        "  #print(subset.head())\n",
        "  for date in nan_dates:\n",
        "    #Find NaN date\n",
        "    nan_index = subset.index[subset['date'] == date].tolist()[0]\n",
        "    nan_date = datetime.strptime(date,'%Y-%m-%d')\n",
        "    \n",
        "    #There is a conditional needed for stations that stopped reporting before 2019\n",
        "    try:\n",
        "      count=0\n",
        "      #Find older date that HAS value. Sometimes needed because filler inserted NaNs\n",
        "      while subset.iloc[nan_index-1-count].isnull().any():\n",
        "        count+=1\n",
        "      #Older date (nan-1)\n",
        "      if (nan_index-1-count)>=0:\n",
        "        older_date = datetime.strptime(subset.iloc[nan_index-1-count]['date'],'%Y-%m-%d')\n",
        "        older_swe = subset.iloc[nan_index-1-count]['SWE']\n",
        "      else:\n",
        "        older_date = datetime.strptime(subset.iloc[nan_index-1]['date'],'%Y-%m-%d')\n",
        "        older_swe = np.nan\n",
        "      #print('Older',nan_index-1,older_date,older_swe)\n",
        "      #print('NaN-inserted',nan_index,nan_date)\n",
        "\n",
        "      #Newer date is next date that HAS value, otherwise enter except\n",
        "      counter=0\n",
        "      while subset.iloc[nan_index+1+counter].isnull().any():\n",
        "        counter+=1\n",
        "      #Newer date\n",
        "      newer_date = datetime.strptime(subset.iloc[nan_index+1+counter]['date'],'%Y-%m-%d')\n",
        "      newer_swe = subset.iloc[nan_index+1+counter]['SWE']\n",
        "      #print('newer',nan_index+1+counter,newer_date,newer_swe)\n",
        "      #print('______________________________')\n",
        "\n",
        "      #Change per day\n",
        "      delta_day = (newer_swe-older_swe)/(newer_date-older_date).days\n",
        "\n",
        "      #Add expected change to older swe\n",
        "      est_swe = older_swe + (delta_day*(nan_date-older_date).days)\n",
        "\n",
        "      #Add \"entry\" row to supplement\n",
        "      supplement.append([station,date,est_swe])\n",
        "    #IndexError happens when the last date is actually from the nan list. Because of this, We DEFINITELY need to do some inter-station interpolation\n",
        "    except IndexError:\n",
        "      supplement.append([station,date,np.nan])\n",
        "\n",
        "#Problem with simple linear interpolation: There are large enough gaps that the \"missing days\" in the data sometimes are the closest dates to themselves"
      ],
      "metadata": {
        "id": "20uFlk86YCcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add incomplete supplement (testing to see how many knn nans are filled in)\n",
        "trainfeatures = trainfeatures.append(pd.DataFrame(supplement, columns=['station_id','date','SWE'])).sort_values(by='date').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "u8_40htVYjCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2_EMS6rFL5k"
      },
      "outputs": [],
      "source": [
        "trainmeta = pd.read_csv(\"/content/drive/MyDrive/snowcapstone team spring 2022/Competition_Data/ground_measures_metadata.csv\")\n",
        "trainmeta.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoHOpJocFcCd"
      },
      "outputs": [],
      "source": [
        "trainfeatures = trainfeatures.merge(trainmeta, how = 'left', on='station_id')\n",
        "trainfeatures.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrRzKQu9G8pL"
      },
      "outputs": [],
      "source": [
        "gridcells = gpd.read_file('/content/drive/MyDrive/snowcapstone team spring 2022/Competition_Data/grid_cells.geojson')\n",
        "print(gridcells.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBFn6NWaOnlu"
      },
      "outputs": [],
      "source": [
        "traindf = pd.read_csv(\"/content/drive/MyDrive/snowcapstone team spring 2022/Competition_Data/train_labels.csv\")\n",
        "\n",
        "traindf = traindf.melt(id_vars=[\"cell_id\"]).dropna().reset_index(drop = True)\n",
        "traindf.rename(columns = {\"cell_id\":\"cell_id\", \"variable\":\"date\", \"value\":\"SWE\"}, inplace = True)\n",
        "\n",
        "traindf = traindf.merge(gridcells, how = 'left', on='cell_id')\n",
        "\n",
        "\n",
        "traindf = gpd.GeoDataFrame(traindf, crs =\"EPSG:4326\")\n",
        "traindf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(len(traindf.loc[pd.to_datetime(traindf.date) > datetime.strptime(\"2016-01-01\", \"%Y-%m-%d\"), \"region\"]))\n",
        "print(len(traindf.loc[traindf[\"region\"] == \"sierras\", \"region\"]))"
      ],
      "metadata": {
        "id": "GKxVUMLeMgiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX-A4GznjM1D"
      },
      "outputs": [],
      "source": [
        "print(len(traindf))\n",
        "traindf = traindf.loc[pd.to_datetime(traindf.date) >= datetime.strptime(\"2016-01-01\", \"%Y-%m-%d\")].reset_index(drop = True)\n",
        "print(len(traindf[\"region\"]))\n",
        "traindf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-HHxU28O1j1"
      },
      "outputs": [],
      "source": [
        "gdf = gpd.GeoDataFrame(trainmeta, \n",
        "                       geometry = gpd.points_from_xy(trainmeta.longitude, trainmeta.latitude),\n",
        "                       crs = \"EPSG:4326\")\n",
        "\n",
        "gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxpooIsbdNFY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#fig, ax = plt.subplots(figsize = (240,240))\n",
        "#\n",
        "#states = gpd.read_file('/content/drive/MyDrive/snowcapstone team spring 2022/Competition_Data/USMap/cb_2018_us_state_20m.shp')\n",
        "#states = states.to_crs(\"EPSG:4326\")\n",
        "#states = states[states['STUSPS'].isin(['WA', 'OR', 'CA', 'NV', 'MT', 'ID', 'WY', 'NM', 'CO' ,'UT', 'AZ'])]\n",
        "#statemap = states.boundary.plot(ax=ax, linewidth=5, zorder = 1)\n",
        "#\n",
        "#gpd.GeoDataFrame(traindf[\"geometry\"]).to_crs(states.crs).plot(ax=ax, facecolor=\"none\", edgecolor='grey')\n",
        "#\n",
        "#gdf[\"geometry\"].plot(ax = ax, markersize = 200, color = 'red',marker = '*', zorder = 2)\n",
        "#\n",
        "#plt.autoscale(False)\n",
        "#ax.axis(\"off\")\n",
        "#\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM2V6oLfIRBk"
      },
      "source": [
        "# Adding Station Data\n",
        "\n",
        "In this section we will take the measurements of ground stations and add those as features to our data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainfeatures = trainfeatures[['station_id',\t'date',\t'SWE',\t'name']]"
      ],
      "metadata": {
        "id": "ieW9vdPnTcGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balltree/KNN approach"
      ],
      "metadata": {
        "id": "C-2zyvRcpeip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "#Adapted from AutoGIS| University of Helsinki\n",
        "# https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n",
        "def get_knearest(src_points, candidates, knn=1):\n",
        "  '''\n",
        "  K nearest neighbors for every source point given candidate points\n",
        "  '''\n",
        "  #Make candidates BallTree format\n",
        "  tree = BallTree(candidates,leaf_size=15,metric='haversine')\n",
        "\n",
        "  #Find closest points\n",
        "  distances, indices = tree.query(src_points, k=knn)\n",
        "\n",
        "  #Transpose into arrays\n",
        "  distances = distances.transpose()\n",
        "  indices = indices.transpose()\n",
        "\n",
        "  #neighbor_idx = []\n",
        "  #neighbor_dist = []\n",
        "   \n",
        "  return(indices, distances)\n",
        "  #Iterate for k neighbors\n",
        "  #for i in range(knn):\n",
        "  #  neighbor_idx.append(indices[i])\n",
        "  #  neighbor_dist.append(distances[i])\n",
        "  #Return list of lists in order of KNN\n",
        "  #return(neighbor_idx,neighbor_dist)\n",
        "\n",
        "\n",
        "\n",
        "  return distances,indices\n",
        "\n",
        "def nearest_neighbor(left_gdf, right_gdf, return_dist=False, knn=1):\n",
        "  \"\"\"\n",
        "  For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n",
        "\n",
        "  NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n",
        "  \"\"\"\n",
        "  #Some Nan buffer to KNN search\n",
        "  knn = knn*3\n",
        "\n",
        "  left_geom_col = left_gdf.geometry.name\n",
        "  right_geom_col = right_gdf.geometry.name\n",
        "\n",
        "  # Ensure that index in right gdf is formed of sequential numbers\n",
        "  right = right_gdf.copy().reset_index(drop=True)\n",
        "\n",
        "  # Parse coordinates from points and insert them into a numpy array as RADIANS\n",
        "  # For left radians, data is in polygon format, so apply meter crs, get centroid, and revert\n",
        "  left_radians = np.array(left_gdf[left_geom_col].to_crs('epsg:4087').centroid.to_crs(\"EPSG:4326\").apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
        "  right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
        "\n",
        "\n",
        "  # Find the nearest points\n",
        "  # -----------------------\n",
        "  # closest ==> index in right_gdf that corresponds to the closest point\n",
        "  # dist ==> distance between the nearest neighbors (in meters)\n",
        "\n",
        "  closest, dist = get_knearest(src_points=left_radians, candidates=right_radians, knn=knn)\n",
        "\n",
        "  #return(closest,dist)\n",
        "    \n",
        "  closest_points = gpd.GeoDataFrame()\n",
        "    \n",
        "  #Loop for knn\n",
        "  for i in range(knn):\n",
        "    # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n",
        "    #Loop to return closest starting from 0 idx\n",
        "    closest_points['station_id_'+str(i)] = right['station_id'].loc[closest[i]].values\n",
        "    closest_points['elevation_m_'+str(i)] = right['elevation_m'].loc[closest[i]].values\n",
        "\n",
        "    # Add distance if requested\n",
        "    if return_dist:\n",
        "      # Convert to meters from radians\n",
        "      earth_radius = 6371000  # meters\n",
        "      closest_points['distance_'+str(i)] = dist[i] * earth_radius\n",
        "\n",
        "  return closest_points\n",
        "\n",
        "def inverseDmean(df,power):\n",
        "  #Formula for inverse Distance Average = ((x1/d1^p)+(x2/d2^p)....)/((1/d1^p)+(1/d2^p)....)\n",
        "  #https://gisgeography.com/inverse-distance-weighting-idw-interpolation/\n",
        "  subset = df.filter(regex='distance_[0-9]+|SWE_[0-9]+')\n",
        "  numerator = pd.DataFrame()\n",
        "  denominator = pd.DataFrame()\n",
        "\n",
        "  for i in range(int(subset.shape[1]/2)):\n",
        "    numerator['x_'+str(i)]=subset['SWE_'+str(i)]/(subset['distance_'+str(i)]**power)\n",
        "    denominator['x_'+str(i)]=1/(subset['distance_'+str(i)]**power)\n",
        "\n",
        "  #There are cells without SWE data. We do not want this in the inverse Distance Calculation\n",
        "  nulls = np.where(pd.isnull(numerator))\n",
        "  for row,column in zip(nulls[0],nulls[1]):\n",
        "    denominator.at[row, denominator.columns[column]] = np.nan\n",
        "  \n",
        "  numerator['sum']=numerator.sum(axis=1)\n",
        "  #print(numerator.head())\n",
        "  denominator['sum']=denominator.sum(axis=1)\n",
        "  #print(denominator.head())\n",
        "\n",
        "  return(numerator['sum']/denominator['sum'])\n",
        "\n",
        "def swe_calculation(train, labels, closest_stations, knn=1):\n",
        "  #Join labels with closest_stations\n",
        "  labels_joined = labels.join(closest_stations)\n",
        "\n",
        "  #Prepare column names\n",
        "  SWE_names=[]\n",
        "  elevation_names=[]\n",
        "  reordered_columns = ['cell_id', 'date', 'SWE', 'region', 'geometry',\n",
        "                       'mean_inversed_swe', 'mean_local_swe',\t'median_local_swe',\t'max_local_swe', 'min_local_swe',\n",
        "                       'mean_local_elevation',\t'median_local_elevation',\t'max_local_elevation','min_local_elevation']\n",
        "  for i in range(knn):\n",
        "    reordered_columns.extend(['station_id_'+str(i),'elevation_m_'+str(i),'distance_'+str(i),'SWE_'+str(i)])\n",
        "    SWE_names.append('SWE_'+str(i))\n",
        "    elevation_names.append('elevation_m_'+str(i))\n",
        "  \n",
        "  #Merge against cell_id+date to get closest stations for each cell\n",
        "  idx = 0\n",
        "  for i in range(knn*3):\n",
        "    train\n",
        "    if i == 0:\n",
        "      tmp_merged = pd.merge(labels_joined, train, how=\"left\", left_on=['station_id_'+str(i), 'date'], right_on=['station_id','date'],suffixes=(None,'_'+str(i))).drop(columns= ['station_id'])\n",
        "    else:\n",
        "      tmp_merged = pd.merge(tmp_merged, train, how=\"left\", left_on=['station_id_'+str(i), 'date'], right_on=['station_id','date'],suffixes=(None,'_'+str(i))).drop(columns= ['station_id'])\n",
        "\n",
        "  #Filter out nearest neighbors with NaN, get 5 closest WITH VALUES\n",
        "  filtered = []\n",
        "  for idx,row in tmp_merged.iterrows():\n",
        "    index = []\n",
        "    values = []\n",
        "    i=0\n",
        "    counter=0\n",
        "    while i<knn:\n",
        "      if not pd.isna(row['SWE_'+str(counter)]):\n",
        "        i+=1\n",
        "        index.append(counter)\n",
        "      counter+=1\n",
        "    for j in index:\n",
        "      values.extend([row['station_id_'+str(j)], row['elevation_m_'+str(j)], row['distance_'+str(j)], row['SWE_'+str(j)]])\n",
        "    filtered.append(values)\n",
        "\n",
        "  #Re-merge with cell data\n",
        "  merged_train = labels.join(pd.DataFrame(filtered,columns=reordered_columns[-4*knn:]))\n",
        "\n",
        "  #Calculations\n",
        "  #Elevations\n",
        "  # Normal Mean\n",
        "  merged_train['mean_local_elevation']=merged_train[elevation_names].mean(axis=1)\n",
        "  # Median\n",
        "  merged_train['median_local_elevation']=merged_train[elevation_names].median(axis=1)\n",
        "  # Max\n",
        "  merged_train['max_local_elevation']=merged_train[elevation_names].max(axis=1)\n",
        "  # Min\n",
        "  merged_train['min_local_elevation']=merged_train[elevation_names].min(axis=1)\n",
        "\n",
        "  #SWE\n",
        "  #Inverse Distance Mean\n",
        "  merged_train['mean_inversed_swe']=inverseDmean(merged_train,2)\n",
        "  #Normal Mean\n",
        "  merged_train['mean_local_swe']=merged_train[SWE_names].mean(axis=1)\n",
        "  #Median\n",
        "  merged_train['median_local_swe']=merged_train[SWE_names].median(axis=1)\n",
        "  #Min\n",
        "  merged_train['min_local_swe']=merged_train[SWE_names].min(axis=1)\n",
        "  #Max\n",
        "  merged_train['max_local_swe']=merged_train[SWE_names].max(axis=1)\n",
        "\n",
        "  #Reorder Columns\n",
        "  merged_train=merged_train[reordered_columns]\n",
        "\n",
        "  return(merged_train)"
      ],
      "metadata": {
        "id": "-RusgZofpduJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn=5\n",
        "#DO NOT want traindf.date or trainfeatures in datetime format\n",
        "\n",
        "closest_stations = nearest_neighbor(traindf, gdf, return_dist=True,knn=knn)\n",
        "traindf = swe_calculation(train=trainfeatures, labels=traindf, closest_stations=closest_stations, knn=knn)"
      ],
      "metadata": {
        "id": "LH4_ggwkrja0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindf.sample(frac = .1).head()"
      ],
      "metadata": {
        "id": "mEob6VOmvsnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindf = traindf[['cell_id','date','SWE','region','geometry','mean_inversed_swe',\n",
        "                   'mean_local_swe','median_local_swe','max_local_swe','min_local_swe',\n",
        "                   'mean_local_elevation','median_local_elevation','max_local_elevation','min_local_elevation',]]\n",
        "                   \n",
        "print(traindf.isna().sum())"
      ],
      "metadata": {
        "id": "fz4DAKdyzUq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udf7IEQnIMOe"
      },
      "source": [
        "# Modis Data\n",
        "\n",
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import requests\n",
        "import ee\n",
        "from datetime import datetime, timedelta\n",
        "import signal\n",
        "\n",
        "class TimeoutException(Exception):   # Custom exception class\n",
        "    pass\n",
        "\n",
        "def timeout_handler(signum, frame):   # Custom signal handler\n",
        "    raise TimeoutException\n",
        "\n",
        "signal.signal(signal.SIGALRM, timeout_handler)\n",
        "\n",
        "traindf[\"date\"] = pd.to_datetime(traindf.date)\n",
        "trainfeatures['date'] = pd.to_datetime(trainfeatures.date)\n",
        "\n",
        "#I am creating a string version of the date to use as a filename\n",
        "traindf[\"datestring\"] = traindf.date.map(lambda d: str(d.year)+d.strftime('%j'))\n",
        "\n",
        "#Now I calculate my centroid from the provided geometry\n",
        "#Ignore the warnings this creates. It is in a projected crs\n",
        "traindf[\"centroid\"] = traindf.geometry.to_crs('+proj=cea').centroid\n",
        "traindf[\"center_lat\"] = traindf.centroid.y\n",
        "traindf[\"center_long\"] = traindf.centroid.x\n",
        "\n",
        "#Logging in to Earth Engine\n",
        "try:\n",
        "        ee.Initialize()\n",
        "except Exception as e:\n",
        "        ee.Authenticate()\n",
        "        ee.Initialize()"
      ],
      "metadata": {
        "id": "OqbPeNPdaI1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def pull_MODIS(traindf, modis, overwrite = False, names_only = False):\n",
        "  filelocations = []\n",
        "  x = 0\n",
        "\n",
        "  for i in range(len(traindf.SWE)):\n",
        "\n",
        "    #create a name for the image\n",
        "    pict_name = traindf.cell_id[i] + '_' + modis + '_' + traindf.datestring[i] + '.jpg'\n",
        "\n",
        "    #create the whole filename with path to the correct folder\n",
        "    filename = os.path.join('/content/', modis, pict_name)\n",
        "\n",
        "    if names_only:\n",
        "      filelocations.append(filename)\n",
        "      x += 1\n",
        "      if x % 5000 == 0:\n",
        "        print(f'{x} files already exist')\n",
        "\n",
        "    elif os.path.exists(filename) and not overwrite:\n",
        "      filelocations.append(filename)\n",
        "      x += 1\n",
        "      if x % 5000 == 0:\n",
        "        print(f'{x} files already exist')\n",
        "\n",
        "    else:\n",
        "      #We need a start date and an end date. Just like a regular python slice, \n",
        "      #the end date is not included, so by using a 1 day frame, I am actually limiting\n",
        "      #the range to only the day in question\n",
        "      start_date = traindf.date[i] - timedelta(days = 7)\n",
        "      end_date = traindf.date[i] + timedelta(days = 1)\n",
        "\n",
        "      #First I get the image collection from the MODIS data, filter it only to the days in question\n",
        "      #and select my bands, then sort so the most recent day in the group is at the top\n",
        "      Collection = ee.ImageCollection(f'MODIS/006/{modis}') \\\n",
        "                  .filter(ee.Filter.date(start_date, end_date)) \\\n",
        "                  .filter(ee.Filter.notNull(['system:index'])) \\\n",
        "                  .select(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', 'NDSI']) \\\n",
        "                  .sort('system:index', False) \n",
        "\n",
        "      #I create a google earth images point based on the area centroid\n",
        "      centroid = ee.Geometry.Point(traindf.center_long[i], traindf.center_lat[i])\n",
        "\n",
        "      #Because the image collection is limited to a single day, there is only one image\n",
        "      #So I just take it\n",
        "      point = Collection.first().unmask(0)\n",
        "\n",
        "      # Get individual band arrays and build them into an RGB image\n",
        "      # The \"buffer\" is a circular distance around the point, measured in meters right now it is 100km\n",
        "      rgb = ee.Image.rgb(point.clip(centroid.buffer(10000)).select('NDSI_Snow_Cover').divide(100), #I divide by 100 to get it between 0 and 1\n",
        "                        point.clip(centroid.buffer(10000)).select('Snow_Albedo_Daily_Tile').divide(100), #I divide by 100 to get it between 0 and 1\n",
        "                        point.clip(centroid.buffer(10000)).select('NDSI').divide(10000)).visualize() #I divide by 10000 to get it between 0 and 1\n",
        "\n",
        "      #Now I get the url for the image\n",
        "      url = rgb.getThumbURL()\n",
        "\n",
        "      #add the name to my list I created earlier\n",
        "      filelocations.append(filename)\n",
        "\n",
        "      #now I open the url and download the image to the specified file location\n",
        "      response = requests.get(url, stream=True)\n",
        "      with open(filename, 'wb') as out_file:\n",
        "          shutil.copyfileobj(response.raw, out_file)\n",
        "      del response\n",
        "    \n",
        "  #traindf[f\"{modis}_filelocations\"] = filelocations\n",
        "'''"
      ],
      "metadata": {
        "id": "uijUxpazQtC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pull_MODIS_list(traindf, modis, signal_timer = 5):\n",
        "  datalist = []\n",
        "  x= 0\n",
        "\n",
        "  still_working = True\n",
        "  while still_working:\n",
        "    try:\n",
        "      Collection = ee.ImageCollection(f'MODIS/006/{modis}') \\\n",
        "                  .select(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', 'NDSI'])\n",
        "      \n",
        "    except Exception as e:\n",
        "      print(\"Some Error with Image Collection\")\n",
        "    else: \n",
        "      signal.alarm(0)\n",
        "      still_working = False        \n",
        "  \n",
        "\n",
        "  for i in range(len(traindf.SWE)):\n",
        "    still_working = True\n",
        "    while still_working:\n",
        "      signal.alarm(signal_timer)\n",
        "      try:\n",
        "        row = [traindf.cell_id[i], traindf.date[i]]\n",
        "\n",
        "        #We need a start date and an end date. Just like a regular python slice, \n",
        "        #the end date is not included, so by using a 1 day frame, I am actually limiting\n",
        "        #the range to only the day in question\n",
        "        start_date = traindf.date[i] - timedelta(days = 7)\n",
        "        end_date = traindf.date[i] + timedelta(days = 1)\n",
        "\n",
        "        #First I get the image collection from the MODIS data, filter it only to the days in question\n",
        "        #and select my bands, then sort so the most recent day in the group is at the top\n",
        "        DatedCollection = Collection.filter(ee.Filter.date(start_date, end_date)) \\\n",
        "                                    .filter(ee.Filter.notNull(['system:index'])) \\\n",
        "                                    .sort('system:index', False)\n",
        "\n",
        "        #Because the image collection is limited to a single day, there is only one image\n",
        "        #So I just take it\n",
        "        point = DatedCollection.first().unmask(0)\n",
        "\n",
        "        aoi = ee.Geometry.Polygon(list(traindf.iloc[i].geometry.exterior.coords))\n",
        "\n",
        "        bands = point.reduceRegion(reducer = ee.Reducer.mean(),\n",
        "        geometry= aoi)\n",
        "\n",
        "        bands = bands.toArray(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', 'NDSI']).getInfo()\n",
        "        bands = np.divide(bands, [100,100,10000] )\n",
        "\n",
        "        row.extend(bands)\n",
        "\n",
        "        datalist.append(row)\n",
        "      \n",
        "      except TimeoutException:\n",
        "        print(f\"Request Timeout for cell_id {traindf.cell_id[i]}\")\n",
        "      \n",
        "      except Exception as e:\n",
        "        print(\"Some other Error\")\n",
        "      else: \n",
        "        signal.alarm(0)\n",
        "        still_working = False\n",
        "        x+=1\n",
        "        if x % 100 == 0:\n",
        "          print(f'{x} out of {len(traindf.SWE)} complete')\n",
        "\n",
        "  data = pd.DataFrame(datalist, columns = ['cell_id', 'date', f'{modis}_SnowCover', \n",
        "                                           f'{modis}_Albedo', f'{modis}_NDSI'])\n",
        "  print(data)\n",
        "  traindf.merge(data, how = 'left', on=['cell_id', 'date'])\n",
        "  return traindf"
      ],
      "metadata": {
        "id": "GaaXWEf7P8MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time "
      ],
      "metadata": {
        "id": "Ud6-utPfU2wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "traindf = pull_MODIS_list(traindf, modis = \"MOD10A1\")\n",
        "print(f'Total Time: {time.time() - start}')\n",
        "print()\n",
        "\n",
        "start = time.time()\n",
        "traindf = pull_MODIS_list(traindf, modis = \"MYD10A1\")\n",
        "print(f'Total Time: {time.time() - start}')\n",
        "\n",
        "\n",
        "#!zip -r '/content/drive/MyDrive/snowcapstone team spring 2022/MODIS_Data/MOD10A1_sierras.zip'  '/content/drive/MyDrive/snowcapstone team spring 2022/MODIS_Data/MOD10A1/'\n",
        "#!zip -r '/content/drive/MyDrive/snowcapstone team spring 2022/MODIS_Data/MYD10A1_sierras.zip'  '/content/drive/MyDrive/snowcapstone team spring 2022/MODIS_Data/MYD10A1/'"
      ],
      "metadata": {
        "id": "JPyROj2bLEQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Copernicus Data"
      ],
      "metadata": {
        "id": "7i_zmwwN4Q5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MATCH FUNCTION"
      ],
      "metadata": {
        "id": "5VIWcB9QjkDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_copernicus(traindf, overwrite = False):\n",
        "  traindf[\"copernicus_filelocations\"] = \"blank\"\n",
        "  x = 0\n",
        "  length_cell_id = len(traindf.cell_id.unique())\n",
        "\n",
        "  for i in traindf.cell_id.unique():\n",
        "    #create a name for the image\n",
        "    pict_name = i + '_' + 'copernicus90m'\n",
        "\n",
        "    #create the whole filename with path to the correct folder\n",
        "    filename = os.path.join('/content/drive/MyDrive/snowcapstone team spring 2022/Copernicus_Data', pict_name)\n",
        "\n",
        "    # Adapted from https://planetarycomputer.microsoft.com/dataset/cop-dem-glo-90#Example-Notebook :\n",
        "    \n",
        "    if not os.path.exists(filename + '.png') or overwrite:\n",
        "      client = Client.open(\n",
        "          \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
        "          ignore_conformance=True,\n",
        "      )\n",
        "\n",
        "      point = [traindf.loc[traindf.cell_id == i, \"center_long\"].iloc[0], \n",
        "              traindf.loc[traindf.cell_id == i, \"center_lat\"].iloc[0]]\n",
        "      \n",
        "      search = client.search(\n",
        "          collections=[\"cop-dem-glo-90\"],\n",
        "          intersects={\"type\": \"Point\", \"coordinates\": point},\n",
        "      )\n",
        "\n",
        "      items = list(search.get_items())\n",
        "\n",
        "      signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n",
        "      \n",
        "      data = (\n",
        "          xarray.open_rasterio(signed_asset.href)\n",
        "          .squeeze()\n",
        "          .drop(\"band\")\n",
        "          .mean()\n",
        "      )\n",
        "      min_lon = min([j[0] for j in [y for y in traindf.loc[traindf.cell_id == i, \"geometry\"].iloc[0].centroid.buffer(.05).boundary.coords]])\n",
        "      min_lat = min([j[1] for j in [y for y in traindf.loc[traindf.cell_id == i, \"geometry\"].iloc[0].centroid.buffer(.05).boundary.coords]])\n",
        "      max_lon = max([j[0] for j in [y for y in traindf.loc[traindf.cell_id == i, \"geometry\"].iloc[0].centroid.buffer(.05).boundary.coords]])\n",
        "      max_lat = max([j[1] for j in [y for y in traindf.loc[traindf.cell_id == i, \"geometry\"].iloc[0].centroid.buffer(.05).boundary.coords]])\n",
        "\n",
        "      mask_lon = (data.x >= min_lon) & (data.x <= max_lon)\n",
        "      mask_lat = (data.y >= min_lat) & (data.x <= max_lat)\n",
        "\n",
        "      cropped_data = data.where(mask_lon & mask_lat, drop=True)\n",
        "\n",
        "      hillshade = xrspatial.hillshade(cropped_data)\n",
        "      img = stack(shade(hillshade, cmap=[\"white\", \"gray\"]), shade(cropped_data, cmap=Elevation, alpha=128))\n",
        "      export_image(img=img, filename=filename, background=None)\n",
        "\n",
        "    traindf.loc[traindf.cell_id == i, \"copernicus_filelocations\"] = filename + '.png'\n",
        "    if x % 500 == 0:\n",
        "      print(f'{x} out of {length_cell_id} complete')\n",
        "    x += 1"
      ],
      "metadata": {
        "id": "MGcd6Xo7_7Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_copernicus(traindf, False)"
      ],
      "metadata": {
        "id": "DuTh-zCfKDNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindf = traindf.drop([\"region\", 'geometry', 'datestring', 'centroid', 'center_lat', 'center_long'], axis = 1)\n",
        "traindf.head()"
      ],
      "metadata": {
        "id": "PJrYQqcVZNpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(traindf).to_csv('/content/drive/MyDrive/snowcapstone team spring 2022/Modeling/traindf_modis_columns_allregions.csv')"
      ],
      "metadata": {
        "id": "352W4g_z6nZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ-aDo6l3llL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Potential Ideas:\n",
        "#Include reverse distance interpolation.\n",
        "#\n",
        "\n",
        "def get_stations(df, num_stations = 5):\n",
        "  #Returns the average SWE from the X closest measurement stations on the same day\n",
        "  averages = []\n",
        "  average_elevation = []\n",
        "  median = []\n",
        "  median_elevation = []\n",
        "  max = []\n",
        "  max_elevation = []\n",
        "  #Calculating distance matrix\n",
        "  dist1 = gpd.GeoDataFrame(geometry = df.geometry.unique(), crs = \"EPSG:4326\").to_crs('+proj=cea')\n",
        "  dist2 = gpd.GeoDataFrame(geometry = trainmeta.geometry, crs = \"EPSG:4326\").to_crs('+proj=cea') \n",
        "\n",
        "  distmatrix = dist2.geometry.apply(lambda g: dist1.geometry.boundary.distance(g))\n",
        "  distmatrix.set_axis(df.cell_id.unique(), axis = 1, inplace = True)\n",
        "  distmatrix[\"station_id\"] = trainmeta.station_id.unique()\n",
        "  \n",
        "  #for each row in the df\n",
        "  for index, row in df.iterrows():\n",
        "    station_dist =distmatrix[[\"station_id\", row.cell_id]]\n",
        "\n",
        "    start_date = row.date - timedelta(days = 7)\n",
        "    end_date = row.date\n",
        "    after_start_date = trainfeatures[\"date\"] >= start_date\n",
        "    before_end_date = trainfeatures[\"date\"] <= end_date\n",
        "    between_two_dates = after_start_date & before_end_date\n",
        "    swes = trainfeatures.loc[between_two_dates]\n",
        "\n",
        "    station_swe = station_dist.merge(swes, how = 'left', on='station_id')\n",
        "    station_swe = station_swe[station_swe[\"SWE\"].notna()]\n",
        "\n",
        "    station_swe.sort_values(by = [row.cell_id, \"date\"], inplace = True)\n",
        "\n",
        "    #This is where interpolation can happen\n",
        "\n",
        "    averages.append(station_swe.head(num_stations).SWE.mean())\n",
        "    average_elevation.append(station_swe.head(num_stations).elevation_m.mean())\n",
        "    median.append(station_swe.head(num_stations).SWE.median())\n",
        "    median_elevation.append(station_swe.head(num_stations).elevation_m.median())\n",
        "    max.append(station_swe.head(num_stations).SWE.max())\n",
        "    max_elevation.append(station_swe.head(num_stations).elevation_m.max())\n",
        "\n",
        "  #return averages, median, max, average_elevation, median_elevation, max_elevation\n",
        "  return averages, average_elevation, median, median_elevation, max, max_elevation\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGVnLsQbCyN8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Turns off a bunch of errors\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "averages, average_elevation, median, median_elevation, max, max_elevation = get_stations(traindf)\n",
        "\n",
        "pd.options.mode.chained_assignment = 'warn'\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwDxgDwIo0CR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "traindf[\"mean_local_swe\"] = averages\n",
        "traindf[\"median_local_swe\"] = median\n",
        "traindf[\"max_local_swe\"] = max\n",
        "traindf[\"mean_local_elevation\"] = average_elevation\n",
        "traindf[\"median_local_elevation\"] = median_elevation\n",
        "traindf[\"max_local_elevation\"] = max_elevation\n",
        "\n",
        "print(traindf.isna().sum())\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingesting Sentinel 2 Data"
      ],
      "metadata": {
        "id": "Q8CQuwM2ywQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling in Data by date/1k square and saving as .tiff format"
      ],
      "metadata": {
        "id": "5qlHktGcy4cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Test that ee connection is still live\n",
        "try:\n",
        "        ee.Initialize()\n",
        "except Exception as e:\n",
        "        ee.Authenticate()\n",
        "        ee.Initialize()"
      ],
      "metadata": {
        "id": "Vc5HIf1ezGHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import additional packages (check for duplicates)\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import requests, zipfile, io\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "0lR9GObqzOQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Image Collection\n",
        "\n",
        "Define image selection to Copernicus/S2 or Sentinel 2 data. \n",
        "\n",
        "Available bands and resolution here: https://gisgeography.com/sentinel-2-bands-combinations/\n",
        "\n",
        "The following visualizations are the most applicable to our study:\n",
        "\n",
        "Geology: Bands B12, B11, B2\n",
        "Vegetation: B8-B4/B8+B4\n",
        "\n",
        "Starting with focus on geology bands, this pulls in the B12(R), B11(G), B4(B)\n",
        "\n",
        "B12 and B11 are at 20m resolution, and B4 is at 10m resolution"
      ],
      "metadata": {
        "id": "gZSc2WkSzk_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentinel 2"
      ],
      "metadata": {
        "id": "by4m_1K81-aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining image\n",
        "sen2 = ee.ImageCollection(\"COPERNICUS/S2\").filterDate(startDate, endDate) \n",
        "\n",
        "#selecting bands\n",
        "sen2 = sen2.select([\"B12\",\"B11\",\"B2\"])"
      ],
      "metadata": {
        "id": "lPe39t4w2G2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentinel 1\n"
      ],
      "metadata": {
        "id": "nFUEMt2UUfEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining image\n",
        "\n",
        "sen1 = ee.ImageCollection(\"COPERNICUS/S1_GRD\").filterDate(startDate, endDate) \n",
        "\n",
        "#selecting bands\n",
        "sen1_A = sen1.select([\"HH\",\"HV\",\"angle\"])\n",
        "sen1_B = sen1.select([\"VV\", \"VH\", \"angle\"])"
      ],
      "metadata": {
        "id": "cx9vSjgxUePq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to define image download\n",
        "\n",
        "Create a funciton to download imagery. Takes the centroid from above and size input and pulls images into google drive. \n",
        "\n",
        "Len = total size of image in meters computed as the resolution of the image bands being pulled in, and the number of pixels we want to capture total (224x224). Then create bounding box around the circle to get a square. "
      ],
      "metadata": {
        "id": "FCrcVtj521fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_download(image,point,image_res,n_pixels,folder_name, image_name, storage=\"Drive\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Function to download satellite images from a ee.imageCollection object.\n",
        "    We first generate a bounding box of image_res*n_pixels meters around \"point\",\n",
        "    then we clip that region from the image collection, take the mean image from the collection,\n",
        "    and send that as a task to the Google Earth Engine. \n",
        "    After that, we download the image Google Cloud Storage if storage==\"Cloud\", \n",
        "    to Google Drive if storage==\"Drive\" or to a local folder if storage==\"local\".\n",
        "    \n",
        "    Inputs:\n",
        "    -image= ee.ImageCollection object\n",
        "    -point= ee.Geometry.Point object\n",
        "    -image_res= resolution of the image in meters\n",
        "    -n_pixels= number of pixels to extract on the images\n",
        "    -storage= string indicating if we are storing the images in Google Cloud,Google Drive or locally.\n",
        "              Defaults to local storage.\n",
        "    -folder_name= string with Google Cloud bucket name if storage==\"Cloud\"\n",
        "                  string with the name of a folder in the root of Google Drive if storage==\"Drive\"\n",
        "                  string with the path to the image if storage==\"local\"\n",
        "    -image_name= string with the image_name for the TIFF image.\n",
        "\n",
        "    Output:\n",
        "     When storage==\"Drive\":\n",
        "     -task= an EE task object. we can then use task.status() to check the status of the task.\n",
        "     If the task is completed, we will see a TIFF image in \"folder_name\" with name \"image_name.tif\".\n",
        "     The image has 3 dimensions, where the first 2 are n_pixels, and the 3rd is the number of bands of \"image\".\n",
        "     When storage==\"local\":\n",
        "     -there is no output, but we will see one TIFF file per band of our image in the folder \"folder_name\".\n",
        "    \"\"\"\n",
        "    #generating the box around the point\n",
        "    len=image_res*n_pixels # for sen2, 20 meters * 224 pixels\n",
        "    region= point.buffer(len/2).bounds().getInfo()['coordinates']\n",
        "    #defining the rectangle\n",
        "    coords=np.array(region)\n",
        "    #taking min and maxs of coordinates to define the rectangle\n",
        "    coords=[np.min(coords[:,:,0]), np.min(coords[:,:,1]), np.max(coords[:,:,0]), np.max(coords[:,:,1])]\n",
        "    rectangle=ee.Geometry.Rectangle(coords)\n",
        "    \n",
        "    if storage==\"Drive\":\n",
        "        #generating the export task (dimensions is \"WIDTHxHEIGHT\")\n",
        "        task=ee.batch.Export.image.toDrive(image=image.filterBounds(rectangle).mean(), \n",
        "                            folder=folder_name, \n",
        "                            description=image_name, \n",
        "                            region=str(region), dimensions=str(n_pixels)+\"x\"+str(n_pixels))\n",
        "        #starting the task\n",
        "        task.start()\n",
        "        return task\n",
        "    \n",
        "    if storage==\"local\":\n",
        "        #downloading the image\n",
        "        r=requests.get( image.filterBounds(rectangle).mean().getDownloadURL({\n",
        "                            'name': image_name, \n",
        "                            'region': str(region),\n",
        "                            'dimensions': str(n_pixels)+\"x\"+str(n_pixels)}))\n",
        "        #unzip it to the selected directory\n",
        "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "        z.extractall(folder_name)"
      ],
      "metadata": {
        "id": "WvXDfzOq4VvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "id": "KY3Rcpvm-g6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test image\n",
        "test=ee.Geometry.Point(-120.61888999873261,39.675880337476684)\n",
        "\n",
        "#running function\n",
        "image_download(image=sen2,point=test,image_res=100,n_pixels=224,folder_name='Sen2_Tiff', image_name='test_image', storage=\"local\")"
      ],
      "metadata": {
        "id": "vJnUWTJB5EFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('Sen2_Tiff')"
      ],
      "metadata": {
        "id": "MkvywTLY-V6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate over all of the dataframe"
      ],
      "metadata": {
        "id": "tHmiPzJr-jDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindf.head()"
      ],
      "metadata": {
        "id": "FoXWDfyZ-lUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(traindf)):\n",
        "\n",
        "    #create a name for the image\n",
        "    image_name = traindf.cell_id[i] + '_sentinel2_' + traindf.datestring[i] + '.jpg'\n",
        "\n",
        "    \n",
        "    startDate = traindf.date[i] - timedelta(days = 365)\n",
        "    endDate = traindf.date[i] + timedelta(days = 1)\n",
        "    year = startDate.year\n",
        "    print(year)\n",
        "    print(startDate)\n",
        "    \n",
        "      #defining image\n",
        "\n",
        "    sen2 = ee.ImageCollection(\"COPERNICUS/S2\")\n",
        "      # filter date\n",
        "    sen2 = sen2.filterDate(startDate, endDate) \n",
        "      #applying cloud masking\n",
        "      #selecting bands\n",
        "    sen2 = sen2.select([\"B12\",\"B11\",\"B2\"])\n",
        "    \n",
        "      #I create a google earth images point based on the area centroid\n",
        "    centroid = ee.Geometry.Point(traindf.center_long[i], traindf.center_lat[i])\n",
        "    print(centroid)\n",
        "      \n",
        "    image_download(image=sen2, point=centroid, image_res=20, n_pixels=224, folder_name='Sen2_Tiff', image_name='image_name', storage=\"local\")\n"
      ],
      "metadata": {
        "id": "ZHuBtaXj-pYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(traindf)):\n",
        "\n",
        "    #create a name for the image\n",
        "    image_name = traindf.cell_id[i] + '_sentinel1a_' + traindf.datestring[i] + '.jpg'\n",
        "\n",
        "    \n",
        "    startDate = traindf.date[i] - timedelta(days = 7)\n",
        "    endDate = traindf.date[i] + timedelta(days = 1)\n",
        "    year = startDate.year\n",
        "    print(year)\n",
        "    print(startDate)\n",
        "    \n",
        "\n",
        "      #defining image\n",
        "\n",
        "    sen1 = ee.ImageCollection(\"COPERNICUS/S1_GRD\").filterDate(startDate, endDate) \n",
        "\n",
        "      #selecting bands\n",
        "    sen1_A = sen1.select([\"HH\",\"HV\",\"angle\"])\n",
        "      # filter date\n",
        "\n",
        "    \n",
        "      #I create a google earth images point based on the area centroid\n",
        "    centroid = ee.Geometry.Point(traindf.center_long[i], traindf.center_lat[i]).buffer(10000)\n",
        "    print(centroid)\n",
        "      \n",
        "    image_download(image=sen1_A, point=centroid, image_res=20, n_pixels=224, folder_name='Sen1a_Tiff', image_name='image_name', storage=\"local\")\n"
      ],
      "metadata": {
        "id": "apj8dWab-pbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(traindf)):\n",
        "\n",
        "    #create a name for the image\n",
        "    image_name = traindf.cell_id[i] + '_sentinel1b_' + traindf.datestring[i] + '.jpg'\n",
        "\n",
        "    \n",
        "    startDate = traindf.date[i] - timedelta(days = 7)\n",
        "    endDate = traindf.date[i] + timedelta(days = 1)\n",
        "    year = startDate.year\n",
        "    print(year)\n",
        "    print(startDate)\n",
        "    \n",
        "\n",
        "\n",
        "    sen1 = ee.ImageCollection(\"COPERNICUS/S1_GRD\").filterDate(startDate, endDate) \n",
        "\n",
        "      #selecting bands\n",
        "    sen1_B = sen1.select([\"VV\", \"VH\", \"angle\"])\n",
        "      # filter date\n",
        "\n",
        "    \n",
        "      #I create a google earth images point based on the area centroid\n",
        "    centroid = ee.Geometry.Point(traindf.center_long[i], traindf.center_lat[i])\n",
        "    print(centroid)\n",
        "      \n",
        "    image_download(image=sen1_b, point=centroid, image_res=20, n_pixels=224, folder_name='Sen1b_Tiff', image_name='image_name', storage=\"local\")\n"
      ],
      "metadata": {
        "id": "zCO4jgCyXI2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading in .tiff images and loading max, min, medians to testdf"
      ],
      "metadata": {
        "id": "e3lLrIlYy94Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pull_SENT2(traindf, overwrite = False, names_only = False):\n",
        "  filelocations = []\n",
        "  x = 0\n",
        "\n",
        "  for i in range(len(traindf.SWE)):\n",
        "\n",
        "    #create a name for the image\n",
        "    pict_name = traindf.cell_id[i] + '_sentinel2_' + traindf.datestring[i] + '.jpg'\n",
        "\n",
        "    #create the whole filename with path to the correct folder\n",
        "    filename = os.path.join('/content/drive/MyDrive/snowcapstone team spring 2022/Sen2_Tiff', pict_name)\n",
        "\n",
        "    if names_only:\n",
        "      filelocations.append(filename)\n",
        "      x += 1\n",
        "      if x % 5000 == 0:\n",
        "        print(f'{x} files already exist')\n",
        "\n",
        "    elif os.path.exists(filename) and not overwrite:\n",
        "      filelocations.append(filename)\n",
        "      x += 1\n",
        "      if x % 5000 == 0:\n",
        "        print(f'{x} files already exist')\n",
        "\n",
        "    else:\n",
        "      #We need a start date and an end date. Just like a regular python slice, \n",
        "      #the end date is not included, so by using a 1 day frame, I am actually limiting\n",
        "      #the range to only the day in question\n",
        "      start_date = traindf.date[i] - timedelta(days = 7)\n",
        "      end_date = traindf.date[i] + timedelta(days = 1)\n",
        "\n",
        "      #First I get the image collection from the MODIS data, filter it only to the days in question\n",
        "      #and select my bands, then sort so the most recent day in the group is at the top\n",
        "      Collection = ee.ImageCollection('COPERNICUS/S2') \\\n",
        "                  .filter(ee.Filter.date(start_date, end_date)) \\\n",
        "                  .select(['B11', 'B12', 'B2']) \\\n",
        "\n",
        "      #I create a google earth images point based on the area centroid\n",
        "      centroid = ee.Geometry.Point(traindf.center_long[i], traindf.center_lat[i])\n",
        "\n",
        "      #Because the image collection is limited to a single day, there is only one image\n",
        "      #So I just take it\n",
        "      point = Collection.first().unmask(0)\n",
        "\n",
        "      # Get individual band arrays and build them into an RGB image\n",
        "      # The \"buffer\" is a circular distance around the point, measured in meters right now it is 100km\n",
        "      rgb = ee.Image.rgb(point.clip(centroid.buffer(10000)).select('B11'),\n",
        "                        point.clip(centroid.buffer(10000)).select('B12'),\n",
        "                        point.clip(centroid.buffer(10000)).select('B2'))\n",
        "\n",
        "      #Now I get the url for the image\n",
        "      url = rgb.getThumbURL({'min': -20, 'max': 0})\n",
        "\n",
        "      #add the name to my list I created earlier\n",
        "      filelocations.append(filename)\n",
        "\n",
        "      #now I open the url and download the image to the specified file location\n",
        "      response = requests.get(url, stream=True)\n",
        "      with open(filename, 'wb') as out_file:\n",
        "          shutil.copyfileobj(response.raw, out_file)\n",
        "      del response\n",
        "    \n",
        "  traindf[\"Sentinel2_filelocations\"] = filelocations"
      ],
      "metadata": {
        "id": "hZ7Wfjn2boc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pull_SENT2(traindf, overwrite = True)"
      ],
      "metadata": {
        "id": "317utemDdb28"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SnowCast Showdown Data Wrangling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c213f4cb-ec09-4d0c-afb2-37184e1923df",
   "metadata": {},
   "source": [
    "# GRIDMET Data Ingestion Script\n",
    "\n",
    "This notebook uses the Google Earth Engine API to pull data from GRIDMET: University of Idaho Gridded Surface Meteorological Dataset: https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_GRIDMET\n",
    "\n",
    "This notebook will run from reading a csv through completion and writing of a csv with the following metrics by day: Total Precipiation, Max Temp, Min Temp, Wind Direction, and Wind Max Velocity\n",
    "\n",
    "Current script is created to pull a CSV from a google storage bucket and read in.\n",
    "\n",
    "Columns expected for input are a key/unique ID (here listed as \"Cell_ID\"), geometry of a polygon, and date.\n",
    "Geometry column is checked and converted to geopandas geometry as part of the script. Date column expects a string and as part of the function strips and returns month/day/year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ffdd4-6361-4a91-9a95-8ec0bd2e5a69",
   "metadata": {},
   "source": [
    "### Install Required Libraries etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c1212-03d8-47a8-8fa9-6024fa2d07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import imagecodecs\n",
    "from geotiff import GeoTiff\n",
    "import geopy\n",
    "import geopy.distance as distance\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "import xarray\n",
    "import dask.dataframe as dd\n",
    "import xrspatial\n",
    "from datashader.transfer_functions import shade, stack\n",
    "from datashader.colors import Elevation\n",
    "from datashader.utils import export_image\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "import shutil\n",
    "import requests\n",
    "import ee\n",
    "\n",
    "import time \n",
    "import signal\n",
    "\n",
    "#import rioxarray\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bff269-0542-40ba-b1f4-664dfb94b4fc",
   "metadata": {},
   "source": [
    "### Connect to Google Storage Bucket\n",
    "\n",
    "For other access options, please change/update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2dc16d-43db-41b1-a556-355d1de2c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "#Connect to Google cloud storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "#Define your storage bucket by inserting your main bucket name here\n",
    "bucket = storage_client.get_bucket('GOOGLE_BUCKET_NAME_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d10a16-ada7-40a1-a3a4-07b17fa6b8d3",
   "metadata": {},
   "source": [
    "### Define Main Function\n",
    "\n",
    "This function pulls a single day of data for the given geometry provided. \n",
    "All outputs are rounded to two decimal places.\n",
    "\n",
    "Current script includes options for writing a temporary dataframe for batching, in the event connection to the cloud is unstable and large amounts of data are being pulled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e072a67-9f97-4219-a8b2-e7fc7a27c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_GRIDMET(df, file_name):\n",
    "    \n",
    "    #Define a temporary dataframe that collects row level data. \n",
    "    internal_df = pd.DataFrame(columns = ['cell_id', 'geometry', 'date', 'precip_daily', 'wind_dir_avg','temp_min', 'temp_max', 'wind_vel'])\n",
    "    \n",
    "    #Set variable x and start time to provide progress feedback and monitoring of speed\n",
    "    x= 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Main function loop that moves over all rows of defined dataframe\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        #define area of interest by coordinates in geometry column\n",
    "        aoi = ee.Geometry.Polygon(list(df.geometry[i].exterior.coords))\n",
    "        \n",
    "        #Convert date column to date time in Y/m/d format and set start and end date\n",
    "        start_date = datetime.strptime(df.date[i], '%Y-%m-%d') - timedelta(days=1)\n",
    "        end_date = datetime.strptime(df.date[i], '%Y-%m-%d')\n",
    "\n",
    "        x+=1\n",
    "        \n",
    "        #Try statement will look for image in the GEE library. If there is no image available for given day/geometry, excpet statement below will return all NULLS\n",
    "        #Before running full script, its advised to check if the satellite in question covers your area and timeframe\n",
    "        try:\n",
    "            \n",
    "            #Access image collection for defined daterange, area, and pulling bands of interest only.\n",
    "            lst = ee.ImageCollection('IDAHO_EPSCOR/GRIDMET')\\\n",
    "                .filterDate(start_date, end_date)\\\n",
    "                .filterBounds(aoi)\\\n",
    "                .select('pr', 'th', 'tmmn', 'tmmx', 'vs')\n",
    "\n",
    "            #Pull each band, select the average value, and round to two decimal places\n",
    "            precip = round(lst.mean().sample(aoi, 1000).first().get('pr').getInfo(),2)\n",
    "            wind_dir = round(lst.mean().sample(aoi, 1000).first().get('th').getInfo(),2)\n",
    "            temp_min = round(lst.mean().sample(aoi, 1000).first().get('tmmn').getInfo(),2)\n",
    "            temp_max = round(lst.mean().sample(aoi, 1000).first().get('tmmx').getInfo(),2)\n",
    "            wind_vel = round(lst.mean().sample(aoi, 1000).first().get('vs').getInfo(),2)\n",
    "            \n",
    "            #Write calculated varialbes to the temporary internal dataframe\n",
    "            internal_df.loc[internal_df.shape[0]] = [df.cell_id[i], df.geometry[i], df.date[i], precip, wind_dir, temp_min, temp_max, wind_vel]\n",
    "        \n",
    "        except Exception as e:\n",
    "            internal_df.loc[internal_df.shape[0]] = [df.cell_id[i], df.geometry[i], df.date[i], 'NULL', 'NULL', 'NULL', 'NULL', 'NULL']\n",
    "        \n",
    "        #Progress printer - Update with whatever interval number you'd like\n",
    "        if x % 1000 == 0:\n",
    "            end_time = time.time()\n",
    "            \n",
    "            #BATCH SAVING - if interested in batch saving csv, un-hash the below statement\n",
    "            #internal_df.to_csv(file_name, index = False)\n",
    "            print(x, \"files complete\")\n",
    "            print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "            start_time = end_time\n",
    "            \n",
    "    #Write final dataframe to CSV        \n",
    "    internal_df.to_csv(file_name, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cc401-cb0d-42aa-a185-113625aad8c3",
   "metadata": {},
   "source": [
    "### Read in CSV from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40e4d9-0db3-4460-a536-34c4bf1f1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define the location within your Google Cloud Storage Bucket which you defined above\n",
    "blob = bucket.blob('SUBFOLDER_NAME/CSV_NAME_OF_FILE.csv')\n",
    "blob = blob.download_as_string()\n",
    "blob = blob.decode('utf-8')\n",
    "\n",
    "blob = StringIO(blob)  #tranform bytes to string here\n",
    "\n",
    "#Read in as CSV and check your data structure. You should have the following columns: cell_id, geometry (in polygon form), and date.\n",
    "df = pd.read_csv(blob)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045ba83-7a87-49ac-bb07-e7307a13ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change geometry column to geopandas geometry and confirm datatype is geometry\n",
    "df['geometry']=gpd.GeoSeries.from_wkt(df['geometry'])\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbef8c-200a-4c6a-8831-a2f56f5e33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Connection to Google Earth Engine API. You must have an account.\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "#Run Function over your file and name the output CSV\n",
    "pull_GRIDMET(gdf, file_name='OUTPUT_CSV_NAME_HERE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5160a4-95a7-4ec6-9212-5598ea4594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your csv to the Google Cloud Storage Bucket\n",
    "!gsutil cp 'OUTPUT_CSV_NAME_HERE.csv' 'gs://GOOGLE_BUCKET_NAME_HERE/SUBFOLDER_WRITING_TO/'"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
